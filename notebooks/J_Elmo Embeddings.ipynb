{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras sentiment analysis with Elmo Embeddings\n",
    "\n",
    "One of the recent trends in Natural Language Processing is transfer learning. Transfer learning allows NLP models to learn more from fewer examples. In this notebook, we experiment with so-called [ELMo Embeddings](https://allennlp.org/elmo), a new approach to word embeddings that relies on a large unlabelled text corpus to understand word meaning in context. ELMo Embeddings are available from [Tensorflow Hub](https://alpha.tfhub.dev/google/elmo/2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "Let's first install and import all the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jlheller/home/Technical/repos/nlp-notebooks/nlp/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1745291915.125088 1773960 pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1745291915.125149 1773960 pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "sess = tf.compat.v1.Session()\n",
    "K.set_session(sess)\n",
    "\n",
    "\n",
    "#tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "#hello = tf.constant('Hello, TensorFlow!')\n",
    "#sess = tf.compat.v1.Session()\n",
    "#print(sess.run(hello))\n",
    "\n",
    "#sess = tf.Session()\n",
    "#backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Argument `fetch` = https://alpha.tfhub.dev/google/elmo/2 cannot be interpreted as a Tensor. (The name 'https://alpha.tfhub.dev/google/elmo/2' looks a like a Tensor name, but is not a valid one. Tensor names must be of the form \"<op_name>:<output_index>\".)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/home/Technical/repos/nlp-notebooks/nlp/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:2942\u001b[0m, in \u001b[0;36mGraph._as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   2941\u001b[0m   op_name, out_n \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2942\u001b[0m   out_n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mout_n\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2943\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: '//alpha.tfhub.dev/google/elmo/2'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/home/Technical/repos/nlp-notebooks/nlp/lib/python3.9/site-packages/tensorflow/python/client/session.py:313\u001b[0m, in \u001b[0;36m_ElementFetchMapper.__init__\u001b[0;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 313\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unique_fetches\u001b[38;5;241m.\u001b[39mappend(\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_graph_element\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_operation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/home/Technical/repos/nlp-notebooks/nlp/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:2908\u001b[0m, in \u001b[0;36mGraph.as_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   2907\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m-> 2908\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_as_graph_element_locked\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_operation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/home/Technical/repos/nlp-notebooks/nlp/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:2944\u001b[0m, in \u001b[0;36mGraph._as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   2943\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m-> 2944\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe name \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m looks a like a Tensor name, but is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2945\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot a valid one. Tensor names must be of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2946\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mform \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m<op_name>:<output_index>\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mrepr\u001b[39m(name))\n\u001b[1;32m   2947\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: The name 'https://alpha.tfhub.dev/google/elmo/2' looks a like a Tensor name, but is not a valid one. Tensor names must be of the form \"<op_name>:<output_index>\".",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m elmo_model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mModule()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#elmo_model = hub.Module(\"https://alpha.tfhub.dev/google/elmo/2\", trainable=True)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#sess.run(tf.initialize_all_variables())\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#sess.run(tf.tables_initializer())\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43msess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://alpha.tfhub.dev/google/elmo/2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/home/Technical/repos/nlp-notebooks/nlp/lib/python3.9/site-packages/tensorflow/python/client/session.py:977\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    974\u001b[0m run_metadata_ptr \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_NewBuffer() \u001b[38;5;28;01mif\u001b[39;00m run_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 977\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions_ptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mrun_metadata_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    979\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n\u001b[1;32m    980\u001b[0m     proto_data \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[0;32m~/home/Technical/repos/nlp-notebooks/nlp/lib/python3.9/site-packages/tensorflow/python/client/session.py:1205\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1202\u001b[0m       feed_map[compat\u001b[38;5;241m.\u001b[39mas_bytes(subfeed_t\u001b[38;5;241m.\u001b[39mname)] \u001b[38;5;241m=\u001b[39m (subfeed_t, subfeed_val)\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;66;03m# Create a fetch handler to take care of the structure of fetches.\u001b[39;00m\n\u001b[0;32m-> 1205\u001b[0m fetch_handler \u001b[38;5;241m=\u001b[39m \u001b[43m_FetchHandler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_handles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeed_handles\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;66;03m# Run request and get response.\u001b[39;00m\n\u001b[1;32m   1209\u001b[0m \u001b[38;5;66;03m# We need to keep the returned movers alive for the following _do_run().\u001b[39;00m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;66;03m# These movers are no longer needed when _do_run() completes, and\u001b[39;00m\n\u001b[1;32m   1211\u001b[0m \u001b[38;5;66;03m# are deleted when `movers` goes out of scope when this _run() ends.\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m \u001b[38;5;66;03m# TODO(yuanbyu, keveman): Revisit whether we should just treat feeding\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;66;03m# of a handle from a different device as an error.\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_with_movers(feed_dict_tensor, feed_map)\n",
      "File \u001b[0;32m~/home/Technical/repos/nlp-notebooks/nlp/lib/python3.9/site-packages/tensorflow/python/client/session.py:494\u001b[0m, in \u001b[0;36m_FetchHandler.__init__\u001b[0;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a fetch handler.\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \n\u001b[1;32m    484\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;124;03m    direct feeds.\u001b[39;00m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mas_default():\n\u001b[0;32m--> 494\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetch_mapper \u001b[38;5;241m=\u001b[39m \u001b[43m_FetchMapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfor_fetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetches \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_targets \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/home/Technical/repos/nlp-notebooks/nlp/lib/python3.9/site-packages/tensorflow/python/client/session.py:285\u001b[0m, in \u001b[0;36m_FetchMapper.for_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fetch, tensor_type):\n\u001b[1;32m    284\u001b[0m       fetches, contraction_fn \u001b[38;5;241m=\u001b[39m fetch_fn(fetch)\n\u001b[0;32m--> 285\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ElementFetchMapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontraction_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# Did not find anything.\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArgument `fetch` = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfetch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has invalid type \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    288\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(fetch)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/home/Technical/repos/nlp-notebooks/nlp/lib/python3.9/site-packages/tensorflow/python/client/session.py:320\u001b[0m, in \u001b[0;36m_ElementFetchMapper.__init__\u001b[0;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[1;32m    316\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArgument `fetch` = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfetch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has invalid type \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    317\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(fetch)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m must be a string or Tensor. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    318\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 320\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArgument `fetch` = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfetch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cannot be interpreted as \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    321\u001b[0m                    \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma Tensor. (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    323\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArgument `fetch` = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfetch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cannot be interpreted as \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    324\u001b[0m                    \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma Tensor. (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Argument `fetch` = https://alpha.tfhub.dev/google/elmo/2 cannot be interpreted as a Tensor. (The name 'https://alpha.tfhub.dev/google/elmo/2' looks a like a Tensor name, but is not a valid one. Tensor names must be of the form \"<op_name>:<output_index>\".)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/bilm/char_embed:0 from checkpoint b'/tmp/tfhub_modules/147211ae67f6cee45196ab9725932f31e8151552/variables/variables' with bilm/char_embed\n"
     ]
    }
   ],
   "source": [
    "elmo_model = tf.compat.v1.Module()\n",
    "#elmo_model = hub.Module(\"https://alpha.tfhub.dev/google/elmo/2\", trainable=True)\n",
    "#sess.run(tf.initialize_all_variables())\n",
    "#sess.run(tf.tables_initializer())\n",
    "sess.run(\"https://alpha.tfhub.dev/google/elmo/2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Runs operations and evaluates tensors in `fetches`.\n",
       "\n",
       "This method runs one \"step\" of TensorFlow computation, by\n",
       "running the necessary graph fragment to execute every `Operation`\n",
       "and evaluate every `Tensor` in `fetches`, substituting the values in\n",
       "`feed_dict` for the corresponding input values.\n",
       "\n",
       "The `fetches` argument may be a single graph element, or an arbitrarily\n",
       "nested list, tuple, namedtuple, dict, or OrderedDict containing graph\n",
       "elements at its leaves.  A graph element can be one of the following types:\n",
       "\n",
       "* A `tf.Operation`.\n",
       "  The corresponding fetched value will be `None`.\n",
       "* A `tf.Tensor`.\n",
       "  The corresponding fetched value will be a numpy ndarray containing the\n",
       "  value of that tensor.\n",
       "* A `tf.sparse.SparseTensor`.\n",
       "  The corresponding fetched value will be a\n",
       "  `tf.compat.v1.SparseTensorValue`\n",
       "  containing the value of that sparse tensor.\n",
       "* A `get_tensor_handle` op.  The corresponding fetched value will be a\n",
       "  numpy ndarray containing the handle of that tensor.\n",
       "* A `string` which is the name of a tensor or operation in the graph.\n",
       "\n",
       "The value returned by `run()` has the same shape as the `fetches` argument,\n",
       "where the leaves are replaced by the corresponding values returned by\n",
       "TensorFlow.\n",
       "\n",
       "Example:\n",
       "\n",
       "```python\n",
       "   a = tf.constant([10, 20])\n",
       "   b = tf.constant([1.0, 2.0])\n",
       "   # 'fetches' can be a singleton\n",
       "   v = session.run(a)\n",
       "   # v is the numpy array [10, 20]\n",
       "   # 'fetches' can be a list.\n",
       "   v = session.run([a, b])\n",
       "   # v is a Python list with 2 numpy arrays: the 1-D array [10, 20] and the\n",
       "   # 1-D array [1.0, 2.0]\n",
       "   # 'fetches' can be arbitrary lists, tuples, namedtuple, dicts:\n",
       "   MyData = collections.namedtuple('MyData', ['a', 'b'])\n",
       "   v = session.run({'k1': MyData(a, b), 'k2': [b, a]})\n",
       "   # v is a dict with\n",
       "   # v['k1'] is a MyData namedtuple with 'a' (the numpy array [10, 20]) and\n",
       "   # 'b' (the numpy array [1.0, 2.0])\n",
       "   # v['k2'] is a list with the numpy array [1.0, 2.0] and the numpy array\n",
       "   # [10, 20].\n",
       "```\n",
       "\n",
       "The optional `feed_dict` argument allows the caller to override\n",
       "the value of tensors in the graph. Each key in `feed_dict` can be\n",
       "one of the following types:\n",
       "\n",
       "* If the key is a `tf.Tensor`, the\n",
       "  value may be a Python scalar, string, list, or numpy ndarray\n",
       "  that can be converted to the same `dtype` as that\n",
       "  tensor. Additionally, if the key is a\n",
       "  `tf.compat.v1.placeholder`, the shape of\n",
       "  the value will be checked for compatibility with the placeholder.\n",
       "* If the key is a\n",
       "  `tf.sparse.SparseTensor`,\n",
       "  the value should be a\n",
       "  `tf.compat.v1.SparseTensorValue`.\n",
       "* If the key is a nested tuple of `Tensor`s or `SparseTensor`s, the value\n",
       "  should be a nested tuple with the same structure that maps to their\n",
       "  corresponding values as above.\n",
       "\n",
       "Each value in `feed_dict` must be convertible to a numpy array of the dtype\n",
       "of the corresponding key.\n",
       "\n",
       "The optional `options` argument expects a [`RunOptions`] proto. The options\n",
       "allow controlling the behavior of this particular step (e.g. turning tracing\n",
       "on).\n",
       "\n",
       "The optional `run_metadata` argument expects a [`RunMetadata`] proto. When\n",
       "appropriate, the non-Tensor output of this step will be collected there. For\n",
       "example, when users turn on tracing in `options`, the profiled info will be\n",
       "collected into this argument and passed back.\n",
       "\n",
       "Args:\n",
       "  fetches: A single graph element, a list of graph elements, or a dictionary\n",
       "    whose values are graph elements or lists of graph elements (described\n",
       "    above).\n",
       "  feed_dict: A dictionary that maps graph elements to values (described\n",
       "    above).\n",
       "  options: A [`RunOptions`] protocol buffer\n",
       "  run_metadata: A [`RunMetadata`] protocol buffer\n",
       "\n",
       "Returns:\n",
       "  Either a single value if `fetches` is a single graph element, or\n",
       "  a list of values if `fetches` is a list, or a dictionary with the\n",
       "  same keys as `fetches` if that is a dictionary (described above).\n",
       "  Order in which `fetches` operations are evaluated inside the call\n",
       "  is undefined.\n",
       "\n",
       "Raises:\n",
       "  RuntimeError: If this `Session` is in an invalid state (e.g. has been\n",
       "    closed).\n",
       "  TypeError: If `fetches` or `feed_dict` keys are of an inappropriate type.\n",
       "  ValueError: If `fetches` or `feed_dict` keys are invalid or refer to a\n",
       "    `Tensor` that doesn't exist.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/home/Technical/repos/nlp-notebooks/nlp/lib/python3.9/site-packages/tensorflow/python/client/session.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?sess.run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELMo Embeddings\n",
    "\n",
    "A quick example will illustrate how ELMo Embeddings work. When we pass to our model a list of sentences (either as strings or as lists of tokens), we get back a list of 1024-dimensional embeddings for every sentence. These are the ELMo embeddings of the tokens in the sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = elmo_model(\n",
    "    [\"the cat is on the mat\", \"dogs are in the fog\"],\n",
    "    signature=\"default\",\n",
    "    as_dict=True)[\"elmo\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis\n",
    "\n",
    "In this experiment, we're going to build a simple neural network for sentiment analysis. As our training and test data, we use the IMDB movie reviews that come pre-packaged with Keras. We shuffle the reviews and pad all texts to a maximum length of 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "VOCABULARY_SIZE = 50000\n",
    "INDEX_FROM = 3\n",
    "START_INDEX = 1\n",
    "OOV_INDEX = 2\n",
    "EMBEDDING_DIM = 300\n",
    "SEQ_LENGTH = 500\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(path=\"imdb.npz\",\n",
    "                                                      num_words=VOCABULARY_SIZE,\n",
    "                                                      skip_top=0,\n",
    "                                                      maxlen=None,\n",
    "                                                      seed=113,\n",
    "                                                      start_char=START_INDEX,\n",
    "                                                      oov_char=OOV_INDEX,\n",
    "                                                      index_from=INDEX_FROM)\n",
    "\n",
    "train = list(zip(X_train, y_train))\n",
    "random.shuffle(train)\n",
    "X_train, y_train = zip(*train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=SEQ_LENGTH)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple embeddings\n",
    "\n",
    "For our baseline, we're going to work with standard word embeddings. These map every token to a 300-dimensional embedding, irrespective of the context in which the token occurs. We'll use the English word embeddings from Facebook Research's [MUSE project](https://github.com/facebookresearch/MUSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://s3.amazonaws.com/arrival/embeddings/wiki.multi.en.vec -O /tmp/wiki.multi.en.vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load these embeddings and put the ones we need in an embedding matrix, where their row indices correspond to the token indices that Keras has assigned to the tokens in the IMDB corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_vectors(embedding_file_path):\n",
    "    print(\"Loading vectors from\", embedding_file_path)\n",
    "    embeddings = []\n",
    "    word2id = {}\n",
    "    with open(embedding_file_path, 'r', encoding='utf-8') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            word, emb = line.rstrip().split(' ', 1)\n",
    "            emb = np.fromstring(emb, sep=' ')\n",
    "            assert word not in word2id, 'word found twice'\n",
    "            embeddings.append(emb)\n",
    "            word2id[word] = len(word2id)\n",
    "\n",
    "    embeddings = np.vstack(embeddings)\n",
    "    return embeddings, word2id\n",
    "\n",
    "embeddings_en, embedding_word2id_en = load_vectors(\"/tmp/wiki.multi.en.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(target_word2id, embedding_word2id, embeddings, num_rows, num_columns):\n",
    "    embedding_matrix = np.zeros((num_rows, num_columns))\n",
    "    for word, i in target_word2id.items():\n",
    "        if i >= num_rows:\n",
    "            continue\n",
    "        if word in embedding_word2id: \n",
    "            embedding_matrix[i] = embeddings[embedding_word2id[word]]\n",
    "    return embedding_matrix\n",
    "\n",
    "word2id_en = imdb.get_word_index()\n",
    "word2id_en = {k:(v+INDEX_FROM) for k,v in word2id_en.items()}\n",
    "word2id_en[\"<PAD>\"] = 0\n",
    "word2id_en[\"<START>\"] = START_INDEX\n",
    "word2id_en[\"<UNK>\"] = OOV_INDEX\n",
    "\n",
    "embedding_matrix_en = create_embedding_matrix(word2id_en, embedding_word2id_en, \n",
    "                                              embeddings_en, VOCABULARY_SIZE+INDEX_FROM-1, EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "We build two models for text classification, which are identical apart from the first layer. Our basic model has a simple embedding layer, were tokens are mapped to their static embeddings. Our ELMo model has a more complex first layer, where the static embedding for each token is concatenated to the ELMo embedding for that token in the relevant context. This results in a 1,324-dimensional embedding for each token in context. In both models, this embedding layer is followed by a simple convolution with kernel size 3, a maximum pooling operation, a dense layer, and finally a final layer that predicts the sentiment of each text as a number between 0 and 1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Lambda, Input\n",
    "from keras.layers import Flatten, Concatenate\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "ELMO_EMBEDDING_DIM = 1024\n",
    "\n",
    "def ElmoEmbedding(x):\n",
    "    y = elmo_model(tf.squeeze(x), signature=\"default\", as_dict=True)[\"elmo\"]\n",
    "    return y\n",
    "\n",
    "def create_basic_model():\n",
    "    sequence = Input(shape=(500,))\n",
    "    embedding = Embedding(VOCABULARY_SIZE+INDEX_FROM-1, EMBEDDING_DIM, input_length=SEQ_LENGTH, \n",
    "                              weights=[embedding_matrix_en], trainable=False)(sequence)\n",
    "        \n",
    "    conv = Conv1D(filters=64, kernel_size=3, padding='same', activation='relu')(embedding)\n",
    "    pool = MaxPooling1D(pool_size=SEQ_LENGTH)(conv)\n",
    "    flat = Flatten()(pool)\n",
    "    dense = Dense(250, activation='relu')(flat)\n",
    "    prediction = Dense(1, activation='sigmoid')(dense)\n",
    "\n",
    "    model = Model(inputs=sequence, outputs=prediction)\n",
    "    optimizer = Adam(lr=0.0001, decay=1e-3)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    #print(model.summary())\n",
    "    \n",
    "    return model\n",
    "    \n",
    "    \n",
    "def create_elmo_model(): \n",
    "    token_sequence = Input(shape=(1,), dtype=\"string\", name=\"elmo_input\")\n",
    "    index_sequence = Input(shape=(SEQ_LENGTH,), name=\"standard_input\")\n",
    "\n",
    "    embedding1 = Lambda(ElmoEmbedding, output_shape=(SEQ_LENGTH, ELMO_EMBEDDING_DIM,))(token_sequence)\n",
    "    embedding2 = Embedding(VOCABULARY_SIZE+INDEX_FROM-1, EMBEDDING_DIM, input_length=SEQ_LENGTH, \n",
    "                          weights=[embedding_matrix_en], trainable=False)(index_sequence)\n",
    "    embedding = Concatenate()([embedding1, embedding2])\n",
    "        \n",
    "    conv = Conv1D(filters=64, kernel_size=3, padding='same', activation='relu')(embedding)\n",
    "    pool = MaxPooling1D(pool_size=SEQ_LENGTH)(conv)\n",
    "    flat = Flatten()(pool)\n",
    "    dense = Dense(250, activation='relu')(flat)\n",
    "    prediction = Dense(1, activation='sigmoid')(dense)\n",
    "\n",
    "    model = Model(inputs=[index_sequence, token_sequence], outputs=prediction)\n",
    "    optimizer = Adam(lr=0.00001, decay=1e-3)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    #print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We train both models for a maximum of 100 epochs, but we stop earlier when the validation loss hasn't improved for two epochs. We save and evaluate the model with the lowest validation loss. Although we didn't make a big effort to tune the learning rate, we did find that the ELMo model benefits from having a much smaller initial learning rate than the basic model. We use the same decay rate for both models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def train_basic_model(model, X_train, y_train, X_val, y_val, X_test, y_test): \n",
    "    batch_size = 16\n",
    "    earlystop = EarlyStopping(monitor='val_loss', patience=2) \n",
    "    checkpoint = ModelCheckpoint('basic_model.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "    model.fit(X_train, y_train, validation_data=(X_val, y_val), \n",
    "              epochs=100, batch_size=batch_size, callbacks=[earlystop, checkpoint])\n",
    "    model.load_weights(filepath='basic_model.hdf5')\n",
    "    scores = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "    print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "    return scores[1]*100\n",
    "\n",
    "\n",
    "def train_elmo_model(model, X_train, E_train, y_train, X_val, E_val, y_val, X_test, E_test, y_test): \n",
    "    batch_size = 16\n",
    "    earlystop = EarlyStopping(monitor='val_loss', patience=2)        \n",
    "    checkpoint = ModelCheckpoint('elmo_model.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "    model.fit([X_train, E_train], y_train, validation_data=([X_val, E_val], y_val), \n",
    "              epochs=100, batch_size=batch_size, callbacks=[earlystop, checkpoint])\n",
    "    model.load_weights(filepath='elmo_model.hdf5')\n",
    "    scores = model.evaluate([X_test, E_test], y_test, batch_size=batch_size)\n",
    "    print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "    return scores[1]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the ELMo model is quite slow, we chose to work with relatively small datasets. We train on just 200 training examples, validate the model on another 200 examples after each epoch, and test its final performance on 500 test examples. We repeat this process 10 times, and choose the training and validation examples randomly from the larger IMDB training set on each iteration. The ELMo model is trained, validated and tested on exactly the same examples as the basic model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0062 - acc: 1.0000 - val_loss: 0.4814 - val_acc: 0.7750\n",
      "Epoch 18/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0052 - acc: 1.0000 - val_loss: 0.4898 - val_acc: 0.7500\n",
      "Epoch 19/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0044 - acc: 1.0000 - val_loss: 0.4761 - val_acc: 0.7800\n",
      "Epoch 20/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0039 - acc: 1.0000 - val_loss: 0.4827 - val_acc: 0.7550\n",
      "Epoch 21/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0033 - acc: 1.0000 - val_loss: 0.4768 - val_acc: 0.7750\n",
      "500/500 [==============================] - 0s 463us/step\n",
      "Accuracy: 77.60%\n",
      "Train on 200 samples, validate on 200 samples\n",
      "Epoch 1/100\n",
      "200/200 [==============================] - 137s 686ms/step - loss: 0.7365 - acc: 0.5700 - val_loss: 0.7008 - val_acc: 0.5450\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 119s 595ms/step - loss: 0.4214 - acc: 0.9000 - val_loss: 0.6105 - val_acc: 0.6400\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 119s 596ms/step - loss: 0.2360 - acc: 0.9500 - val_loss: 0.5315 - val_acc: 0.7400\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 119s 596ms/step - loss: 0.0874 - acc: 1.0000 - val_loss: 0.5177 - val_acc: 0.7650\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 119s 597ms/step - loss: 0.0292 - acc: 1.0000 - val_loss: 0.4674 - val_acc: 0.7700\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 119s 597ms/step - loss: 0.0104 - acc: 1.0000 - val_loss: 0.4516 - val_acc: 0.7850\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 119s 595ms/step - loss: 0.0060 - acc: 1.0000 - val_loss: 0.4527 - val_acc: 0.7950\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 119s 594ms/step - loss: 0.0033 - acc: 1.0000 - val_loss: 0.4443 - val_acc: 0.7850\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 119s 595ms/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.4411 - val_acc: 0.7850\n",
      "Epoch 10/100\n",
      "200/200 [==============================] - 119s 594ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.4484 - val_acc: 0.7850\n",
      "Epoch 11/100\n",
      "200/200 [==============================] - 119s 595ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.4512 - val_acc: 0.7800\n",
      "500/500 [==============================] - 147s 294ms/step\n",
      "Accuracy: 76.80%\n",
      "[73.2, 73.6, 52.400000000000006, 47.599999999999994, 63.6, 52.6, 77.60000000000001]\n",
      "[78.0, 75.4, 75.4, 77.8, 75.6, 72.6, 76.8]\n",
      "Train on 200 samples, validate on 200 samples\n",
      "Epoch 1/100\n",
      "200/200 [==============================] - 19s 93ms/step - loss: 0.6901 - acc: 0.5300 - val_loss: 0.7155 - val_acc: 0.4850\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.6818 - acc: 0.5450 - val_loss: 0.7007 - val_acc: 0.4850\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.6619 - acc: 0.8350 - val_loss: 0.6900 - val_acc: 0.5050\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.6344 - acc: 0.5600 - val_loss: 0.6899 - val_acc: 0.4800\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.5892 - acc: 0.9050 - val_loss: 0.6838 - val_acc: 0.5000\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.5019 - acc: 0.9600 - val_loss: 0.6536 - val_acc: 0.6100\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.4113 - acc: 0.9500 - val_loss: 0.6431 - val_acc: 0.6050\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2858 - acc: 0.9750 - val_loss: 0.6098 - val_acc: 0.6850\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1752 - acc: 0.9950 - val_loss: 0.5943 - val_acc: 0.7050\n",
      "Epoch 10/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1000 - acc: 1.0000 - val_loss: 0.5566 - val_acc: 0.7550\n",
      "Epoch 11/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0691 - acc: 1.0000 - val_loss: 0.5453 - val_acc: 0.7450\n",
      "Epoch 12/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0398 - acc: 1.0000 - val_loss: 0.5580 - val_acc: 0.6700\n",
      "Epoch 13/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0243 - acc: 1.0000 - val_loss: 0.5394 - val_acc: 0.7600\n",
      "Epoch 14/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0151 - acc: 1.0000 - val_loss: 0.5572 - val_acc: 0.7400\n",
      "Epoch 15/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0111 - acc: 1.0000 - val_loss: 0.5395 - val_acc: 0.7600\n",
      "500/500 [==============================] - 0s 445us/step\n",
      "Accuracy: 70.40%\n",
      "Train on 200 samples, validate on 200 samples\n",
      "Epoch 1/100\n",
      "200/200 [==============================] - 138s 691ms/step - loss: 0.7729 - acc: 0.5050 - val_loss: 0.7021 - val_acc: 0.5200\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 119s 595ms/step - loss: 0.5565 - acc: 0.7350 - val_loss: 0.6076 - val_acc: 0.7000\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 119s 595ms/step - loss: 0.3504 - acc: 0.9100 - val_loss: 0.7633 - val_acc: 0.5450\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 119s 595ms/step - loss: 0.1997 - acc: 0.9550 - val_loss: 0.5112 - val_acc: 0.7300\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 119s 596ms/step - loss: 0.0722 - acc: 1.0000 - val_loss: 0.5036 - val_acc: 0.7650\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 119s 595ms/step - loss: 0.0237 - acc: 1.0000 - val_loss: 0.6465 - val_acc: 0.6500\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 119s 594ms/step - loss: 0.0121 - acc: 1.0000 - val_loss: 0.5000 - val_acc: 0.7550\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 119s 595ms/step - loss: 0.0046 - acc: 1.0000 - val_loss: 0.5470 - val_acc: 0.7550\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 119s 594ms/step - loss: 0.0029 - acc: 1.0000 - val_loss: 0.5003 - val_acc: 0.7350\n",
      "500/500 [==============================] - 146s 292ms/step\n",
      "Accuracy: 75.20%\n",
      "[73.2, 73.6, 52.400000000000006, 47.599999999999994, 63.6, 52.6, 77.60000000000001, 70.39999999999999]\n",
      "[78.0, 75.4, 75.4, 77.8, 75.6, 72.6, 76.8, 75.2]\n",
      "Train on 200 samples, validate on 200 samples\n",
      "Epoch 1/100\n",
      "200/200 [==============================] - 19s 97ms/step - loss: 0.7058 - acc: 0.4350 - val_loss: 0.6990 - val_acc: 0.4850\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.6823 - acc: 0.5500 - val_loss: 0.6917 - val_acc: 0.4850\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.6617 - acc: 0.5900 - val_loss: 0.6862 - val_acc: 0.5200\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.6235 - acc: 0.8850 - val_loss: 0.6760 - val_acc: 0.6650\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.5606 - acc: 0.9450 - val_loss: 0.6644 - val_acc: 0.6350\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.4543 - acc: 0.9950 - val_loss: 0.6354 - val_acc: 0.7300\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.3182 - acc: 0.9900 - val_loss: 0.6208 - val_acc: 0.7100\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1841 - acc: 1.0000 - val_loss: 0.5786 - val_acc: 0.7300\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0977 - acc: 1.0000 - val_loss: 0.5662 - val_acc: 0.7300\n",
      "Epoch 10/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0485 - acc: 1.0000 - val_loss: 0.5511 - val_acc: 0.7450\n",
      "Epoch 11/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0291 - acc: 1.0000 - val_loss: 0.5544 - val_acc: 0.7350\n",
      "Epoch 12/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0187 - acc: 1.0000 - val_loss: 0.5564 - val_acc: 0.7250\n",
      "500/500 [==============================] - 0s 450us/step\n",
      "Accuracy: 73.00%\n",
      "Train on 200 samples, validate on 200 samples\n",
      "Epoch 1/100\n",
      "200/200 [==============================] - 139s 696ms/step - loss: 0.7269 - acc: 0.5750 - val_loss: 0.6576 - val_acc: 0.5750\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 119s 595ms/step - loss: 0.4709 - acc: 0.8800 - val_loss: 0.6180 - val_acc: 0.6500\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 119s 596ms/step - loss: 0.2449 - acc: 0.9650 - val_loss: 0.6223 - val_acc: 0.6700\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 119s 596ms/step - loss: 0.0785 - acc: 1.0000 - val_loss: 0.5302 - val_acc: 0.7650\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 119s 597ms/step - loss: 0.0257 - acc: 1.0000 - val_loss: 0.5341 - val_acc: 0.7400\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 119s 596ms/step - loss: 0.0100 - acc: 1.0000 - val_loss: 0.5540 - val_acc: 0.7150\n",
      "500/500 [==============================] - 147s 294ms/step\n",
      "Accuracy: 75.40%\n",
      "[73.2, 73.6, 52.400000000000006, 47.599999999999994, 63.6, 52.6, 77.60000000000001, 70.39999999999999, 73.0]\n",
      "[78.0, 75.4, 75.4, 77.8, 75.6, 72.6, 76.8, 75.2, 75.4]\n",
      "Train on 200 samples, validate on 200 samples\n",
      "Epoch 1/100\n",
      "200/200 [==============================] - 21s 103ms/step - loss: 0.6945 - acc: 0.5300 - val_loss: 0.6905 - val_acc: 0.5200\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.6844 - acc: 0.5300 - val_loss: 0.6885 - val_acc: 0.6350\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.6541 - acc: 0.6250 - val_loss: 0.6828 - val_acc: 0.6850\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.6148 - acc: 0.7900 - val_loss: 0.6729 - val_acc: 0.6900\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.5352 - acc: 0.9950 - val_loss: 0.6590 - val_acc: 0.5650\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.4273 - acc: 0.9900 - val_loss: 0.6433 - val_acc: 0.5750\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.3053 - acc: 0.9950 - val_loss: 0.6134 - val_acc: 0.6800\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1787 - acc: 1.0000 - val_loss: 0.5902 - val_acc: 0.7100\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0959 - acc: 1.0000 - val_loss: 0.5949 - val_acc: 0.6650\n",
      "Epoch 10/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0558 - acc: 1.0000 - val_loss: 0.6092 - val_acc: 0.6300\n",
      "500/500 [==============================] - 0s 449us/step\n",
      "Accuracy: 68.00%\n",
      "Train on 200 samples, validate on 200 samples\n",
      "Epoch 1/100\n",
      "200/200 [==============================] - 140s 701ms/step - loss: 0.7259 - acc: 0.5050 - val_loss: 0.6911 - val_acc: 0.4950\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 119s 597ms/step - loss: 0.5202 - acc: 0.8050 - val_loss: 0.6503 - val_acc: 0.6300\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 119s 597ms/step - loss: 0.2861 - acc: 0.9300 - val_loss: 0.6291 - val_acc: 0.6450\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 119s 597ms/step - loss: 0.1120 - acc: 0.9950 - val_loss: 0.5759 - val_acc: 0.7050\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 119s 597ms/step - loss: 0.0415 - acc: 1.0000 - val_loss: 0.5750 - val_acc: 0.7100\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 120s 598ms/step - loss: 0.0128 - acc: 1.0000 - val_loss: 0.5392 - val_acc: 0.7450\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 119s 596ms/step - loss: 0.0061 - acc: 1.0000 - val_loss: 0.5476 - val_acc: 0.7350\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 119s 596ms/step - loss: 0.0037 - acc: 1.0000 - val_loss: 0.5535 - val_acc: 0.7350\n",
      "500/500 [==============================] - 146s 293ms/step\n",
      "Accuracy: 74.00%\n",
      "[73.2, 73.6, 52.400000000000006, 47.599999999999994, 63.6, 52.6, 77.60000000000001, 70.39999999999999, 73.0, 68.0]\n",
      "[78.0, 75.4, 75.4, 77.8, 75.6, 72.6, 76.8, 75.2, 75.4, 74.0]\n",
      "65.2\n",
      "75.61999999999999\n"
     ]
    }
   ],
   "source": [
    "elmo_accuracies = []\n",
    "basic_accuracies = []\n",
    "\n",
    "test_size = 500\n",
    "validation_size = 200\n",
    "training_size = 200\n",
    "id2word_en = {v:k for k,v in word2id_en.items()}\n",
    "\n",
    "for i in range(10):     \n",
    "    \n",
    "    train = list(zip(X_train, y_train))\n",
    "    test = list(zip(X_test, y_test))\n",
    "    random.shuffle(train)\n",
    "    X_train, y_train = zip(*train)\n",
    "    X_test, y_test = zip(*test)\n",
    "    \n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    \n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "            \n",
    "    train_texts = [\" \".join([id2word_en[idx] for idx in seq]) for seq in X_train[:training_size]]\n",
    "    test_texts = [\" \".join([id2word_en[idx] for idx in seq]) for seq in X_test[:test_size]]\n",
    "    val_texts = [\" \".join([id2word_en[idx] for idx in seq]) for seq in X_test[test_size:test_size+validation_size]]\n",
    "\n",
    "    E_train = np.array(train_texts)\n",
    "    E_test = np.array(test_texts)\n",
    "    E_val = np.array(val_texts)\n",
    "        \n",
    "    model_baseline = create_basic_model()\n",
    "    basic_acc = train_basic_model(model_baseline, X_train[:training_size], y_train[:training_size], \n",
    "                                  X_test[test_size:test_size+validation_size],\n",
    "                                  y_test[test_size:test_size+validation_size], \n",
    "                                  X_test[:test_size], y_test[:test_size])\n",
    "    basic_accuracies.append(basic_acc)\n",
    "\n",
    "    model_elmo = create_elmo_model()\n",
    "    elmo_acc = train_elmo_model(model_elmo, X_train[:training_size], E_train, y_train[:training_size],\n",
    "                                X_test[test_size:test_size+validation_size], \n",
    "                                E_val, y_test[test_size:test_size+validation_size], \n",
    "                                X_test[:test_size], E_test, y_test[:test_size])\n",
    "    elmo_accuracies.append(elmo_acc)\n",
    "        \n",
    "    print(basic_accuracies)\n",
    "    print(elmo_accuracies)\n",
    "    \n",
    "print(np.mean(basic_accuracies))\n",
    "print(np.mean(elmo_accuracies))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "When we train a basic sentiment analysis model on just 200 training examples, the results are hit and miss: the accuracies on unseen texts range from just 48% to 78%. When we replace the simple embedding layer by an ELMo embedding layer, however, the model performs about 10% better on average. Its accuracies were also much more consistent, between 73% and 78%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "accuracies = pd.DataFrame({'basic' : basic_accuracies, 'elmo': elmo_accuracies})\n",
    "plt.rcParams['figure.figsize'] = (10,6)\n",
    "accuracies.boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair plot\n",
    "plt.scatter(np.zeros(len(basic_accuracies)), basic_accuracies)\n",
    "plt.scatter(np.ones(len(elmo_accuracies)), elmo_accuracies)\n",
    "\n",
    "for i in range(len(basic_accuracies)):\n",
    "    plt.plot( [0,1], [basic_accuracies[i], elmo_accuracies[i]], c='k')\n",
    "\n",
    "plt.xticks([0,1], ['basic', 'elmo'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting simple_elmo\n",
      "  Downloading simple_elmo-0.9.2-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: h5py in ./nlp/lib/python3.9/site-packages (from simple_elmo) (3.13.0)\n",
      "Requirement already satisfied: numpy in ./nlp/lib/python3.9/site-packages (from simple_elmo) (1.26.4)\n",
      "Requirement already satisfied: smart-open>1.8.1 in ./nlp/lib/python3.9/site-packages (from simple_elmo) (7.1.0)\n",
      "Requirement already satisfied: pandas in ./nlp/lib/python3.9/site-packages (from simple_elmo) (2.2.3)\n",
      "Requirement already satisfied: scipy in ./nlp/lib/python3.9/site-packages (from simple_elmo) (1.13.1)\n",
      "Requirement already satisfied: wrapt in ./nlp/lib/python3.9/site-packages (from smart-open>1.8.1->simple_elmo) (1.17.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./nlp/lib/python3.9/site-packages (from pandas->simple_elmo) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./nlp/lib/python3.9/site-packages (from pandas->simple_elmo) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./nlp/lib/python3.9/site-packages (from pandas->simple_elmo) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./nlp/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->simple_elmo) (1.17.0)\n",
      "Downloading simple_elmo-0.9.2-py3-none-any.whl (46 kB)\n",
      "Installing collected packages: simple_elmo\n",
      "Successfully installed simple_elmo-0.9.2\n"
     ]
    }
   ],
   "source": [
    "!pip install simple_elmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simple_elmo import ElmoModel\n",
    "model = ElmoModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow_hub' has no attribute 'Module'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m elmo \u001b[38;5;241m=\u001b[39m \u001b[43mhub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModule\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://tfhub.dev/google/elmo/2\u001b[39m\u001b[38;5;124m\"\u001b[39m, trainable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow_hub' has no attribute 'Module'"
     ]
    }
   ],
   "source": [
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo_model = tf.compat.v1.Module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Base neural network module class.\n",
       "\n",
       "A module is a named container for `tf.Variable`s, other `tf.Module`s and\n",
       "functions which apply to user input. For example a dense layer in a neural\n",
       "network might be implemented as a `tf.Module`:\n",
       "\n",
       ">>> class Dense(tf.Module):\n",
       "...   def __init__(self, input_dim, output_size, name=None):\n",
       "...     super().__init__(name=name)\n",
       "...     self.w = tf.Variable(\n",
       "...       tf.random.normal([input_dim, output_size]), name='w')\n",
       "...     self.b = tf.Variable(tf.zeros([output_size]), name='b')\n",
       "...   def __call__(self, x):\n",
       "...     y = tf.matmul(x, self.w) + self.b\n",
       "...     return tf.nn.relu(y)\n",
       "\n",
       "You can use the Dense layer as you would expect:\n",
       "\n",
       ">>> d = Dense(input_dim=3, output_size=2)\n",
       ">>> d(tf.ones([1, 3]))\n",
       "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=..., dtype=float32)>\n",
       "\n",
       "\n",
       "By subclassing `tf.Module` instead of `object` any `tf.Variable` or\n",
       "`tf.Module` instances assigned to object properties can be collected using\n",
       "the `variables`, `trainable_variables` or `submodules` property:\n",
       "\n",
       ">>> d.variables\n",
       "    (<tf.Variable 'b:0' shape=(2,) dtype=float32, numpy=...,\n",
       "    dtype=float32)>,\n",
       "    <tf.Variable 'w:0' shape=(3, 2) dtype=float32, numpy=..., dtype=float32)>)\n",
       "\n",
       "\n",
       "Subclasses of `tf.Module` can also take advantage of the `_flatten` method\n",
       "which can be used to implement tracking of any other types.\n",
       "\n",
       "All `tf.Module` classes have an associated `tf.name_scope` which can be used\n",
       "to group operations in TensorBoard and create hierarchies for variable names\n",
       "which can help with debugging. We suggest using the name scope when creating\n",
       "nested submodules/parameters or for forward methods whose graph you might want\n",
       "to inspect in TensorBoard. You can enter the name scope explicitly using\n",
       "`with self.name_scope:` or you can annotate methods (apart from `__init__`)\n",
       "with `@tf.Module.with_name_scope`.\n",
       "\n",
       ">>> class MLP(tf.Module):\n",
       "...   def __init__(self, input_size, sizes, name=None):\n",
       "...     super().__init__(name=name)\n",
       "...     self.layers = []\n",
       "...     with self.name_scope:\n",
       "...       for size in sizes:\n",
       "...         self.layers.append(Dense(input_dim=input_size, output_size=size))\n",
       "...         input_size = size\n",
       "...   @tf.Module.with_name_scope\n",
       "...   def __call__(self, x):\n",
       "...     for layer in self.layers:\n",
       "...       x = layer(x)\n",
       "...     return x\n",
       "\n",
       ">>> module = MLP(input_size=5, sizes=[5, 5])\n",
       ">>> module.variables\n",
       "(<tf.Variable 'mlp/b:0' shape=(5,) dtype=float32, numpy=..., dtype=float32)>,\n",
       "<tf.Variable 'mlp/w:0' shape=(5, 5) dtype=float32, numpy=...,\n",
       "   dtype=float32)>,\n",
       "<tf.Variable 'mlp/b:0' shape=(5,) dtype=float32, numpy=..., dtype=float32)>,\n",
       "<tf.Variable 'mlp/w:0' shape=(5, 5) dtype=float32, numpy=...,\n",
       "   dtype=float32)>)\n",
       "\u001b[0;31mFile:\u001b[0m           ~/home/Technical/repos/nlp-notebooks/nlp/lib/python3.9/site-packages/tensorflow/python/module/module.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     Layer, VariableAndLossTracker, LinearOperator, Layer"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?tf.compat.v1.Module"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
