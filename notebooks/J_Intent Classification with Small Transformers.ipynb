{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intent Classification with Small Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In recent years, the transformer has been the go-to model in Natural Language Processing for almost all NLP tasks. Pretrained transformer models are easily available, but they are often large and fairly slow, which can make them hard to deploy. That's why a variety of smaller transformer models have emerged, which aim to compete with their bigger rivals, but at a lower computational cost. In this notebook we'll explore some of these smaller transformers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intent classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll evaluate a range of pretrained transformer models on intent classification. Intent classification is a crucial task in the development of chatbots, where the computer needs to figure out how to respond to the input of a user. An interesting example dataset is `banking77`, which is available in the `datasets` library. It contains 13,083 customer service queries from the banking domain: 10,003 for training and 3080 for testing. Since there are 77 intents, it's a fairly challenging dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jlheller/home/Technical/repos/nlp-notebooks/nlp/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "522c4ce5c242496789a500b3e9de39b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/14.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94cadd808f8241d38f648b6690bac391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/298k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "977cbcc1ae2249af861e6316abf470e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/93.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c15cf93b573480dbb10a1a0195420d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/10003 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ed24c937ca5411cac10268f39ba7768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/3080 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 10003\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 3080\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('banking77')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intents cover a wide variety of banking topics, from `pin_blocked` to `declined_transfer` and `terminate_account`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['activate_my_card', 'age_limit', 'apple_pay_or_google_pay', 'atm_support', 'automatic_top_up', 'balance_not_updated_after_bank_transfer', 'balance_not_updated_after_cheque_or_cash_deposit', 'beneficiary_not_allowed', 'cancel_transfer', 'card_about_to_expire', 'card_acceptance', 'card_arrival', 'card_delivery_estimate', 'card_linking', 'card_not_working', 'card_payment_fee_charged', 'card_payment_not_recognised', 'card_payment_wrong_exchange_rate', 'card_swallowed', 'cash_withdrawal_charge', 'cash_withdrawal_not_recognised', 'change_pin', 'compromised_card', 'contactless_not_working', 'country_support', 'declined_card_payment', 'declined_cash_withdrawal', 'declined_transfer', 'direct_debit_payment_not_recognised', 'disposable_card_limits', 'edit_personal_details', 'exchange_charge', 'exchange_rate', 'exchange_via_app', 'extra_charge_on_statement', 'failed_transfer', 'fiat_currency_support', 'get_disposable_virtual_card', 'get_physical_card', 'getting_spare_card', 'getting_virtual_card', 'lost_or_stolen_card', 'lost_or_stolen_phone', 'order_physical_card', 'passcode_forgotten', 'pending_card_payment', 'pending_cash_withdrawal', 'pending_top_up', 'pending_transfer', 'pin_blocked', 'receiving_money', 'Refund_not_showing_up', 'request_refund', 'reverted_card_payment?', 'supported_cards_and_currencies', 'terminate_account', 'top_up_by_bank_transfer_charge', 'top_up_by_card_charge', 'top_up_by_cash_or_cheque', 'top_up_failed', 'top_up_limits', 'top_up_reverted', 'topping_up_by_card', 'transaction_charged_twice', 'transfer_fee_charged', 'transfer_into_account', 'transfer_not_received_by_recipient', 'transfer_timing', 'unable_to_verify_identity', 'verify_my_identity', 'verify_source_of_funds', 'verify_top_up', 'virtual_card_not_working', 'visa_or_mastercard', 'why_verify_identity', 'wrong_amount_of_cash_received', 'wrong_exchange_rate_for_cash_withdrawal'], id=None)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"].features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user inputs themselves are fairly short: they're questions like \"What can I do if my card still hasn't arrived after 2 weeks?\" or \"How do I know if I will get my card, or if it is lost?\" Every item has one label, which makes this a straightforward single-label classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am still waiting on my card?\n",
      "What can I do if my card still hasn't arrived after 2 weeks?\n",
      "I have been waiting over a week. Is the card still coming?\n",
      "Can I track my card while it is in the process of delivery?\n",
      "How do I know if I will get my card, or if it is lost?\n"
     ]
    }
   ],
   "source": [
    "for item in dataset[\"train\"][\"text\"][:5]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the distribution of the labels. Many text classification problems are hard, not only because of the number of labels, but also because of their skewed distribution. Fortunately, the situation in the `banking77` dataset is not too bad: there's a handful of infrequent labels, but overall, the skew in the distribution is not very pronounced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "train_texts = [item[\"text\"] for item in dataset[\"train\"]]\n",
    "train_labels = [item[\"label\"] for item in dataset[\"train\"]]\n",
    "\n",
    "test_texts = [item[\"text\"] for item in dataset[\"test\"]]\n",
    "test_labels = [item[\"label\"] for item in dataset[\"test\"]]\n",
    "\n",
    "label_counter = Counter(train_labels)\n",
    "label_names = dataset[\"train\"].features[\"label\"].names\n",
    "label_frequencies = {label_names[label]: [label_counter[label]] for label in label_counter}\n",
    "\n",
    "df = pd.DataFrame.from_dict(label_frequencies, orient=\"index\", columns=[\"frequency\"])\n",
    "df = df.sort_values(\"frequency\", ascending=False)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10,6)\n",
    "ax = df.transpose().plot(kind=\"bar\", rot=0)\n",
    "ax.get_legend().remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start training, we need to take a few preparatory steps. First of all, we set aside 10% of the training data as our development (or validation) set, which we'll use to evaluate the performance of the models during training. We keep the test set intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 9002\n",
      "Dev: 1001\n",
      "Test: 3080\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_texts, dev_texts, train_labels, dev_labels = train_test_split(train_texts, \n",
    "                                                                    train_labels, \n",
    "                                                                    test_size=0.1, \n",
    "                                                                    shuffle=True, \n",
    "                                                                    random_state=1)\n",
    "\n",
    "print(\"Train:\", len(train_texts))\n",
    "print(\"Dev:\", len(dev_texts))\n",
    "print(\"Test:\", len(test_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a dataset class to wrap the data in. When the model trainer gets an item from this class, it returns all information in the encoding (the token ids, the mask ids, etc.), together with the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class ClassificationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['label'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define the metrics that we'll evaluate the models with. As this is a single-label classification task, we'll simply compute the overall accuracy of the model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smaller transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate two types of small transformer models: ALBERT, and a family of smaller BERT models.\n",
    "    \n",
    "ALBERT was introduced in the paper [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942). It uses various insights to shrink the footprint of a BERT model:\n",
    "\n",
    "- In contrast to models like BERT and XLNet, it decouples the size of the token embeddings from the size of the hidden layer. This allows it to have a much smaller embedding layer, and therefore, a more efficient model.\n",
    "- It further reduces BERT's memory requirements by sharing all parameters across layers. \n",
    "\n",
    "Finally, in pretraining, the original next-sentence prediction task is replaced by a sentence order prediction task, where the model has to decide whether two sentences are presented in their original order, or whether the order has been swapped. This obviously has no effect on the size or speed of the model, but is a more challenging pretraining task that forces the model to focus more on sentence coherence and not just sentence topic.\n",
    "\n",
    "While the original BERT has 108M parameters, ALBERT-base has just 12M and ALBERT-large has 18M."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another type of smaller BERT models was presented in the paper [Well-Read Students Learn Better: On the Importance of Pre-training Compact Models](https://arxiv.org/abs/1908.08962). The researchers combine pretraining with model distillation to train smaller models:\n",
    "\n",
    "- In a first step, a compact model is pretrained using the well-known masked language modeling objective.\n",
    "- In a second step, these small student models are further trained on the label probabilities (the so-called soft labels) that are produced by a larger teacher model.\n",
    "\n",
    "We'll test models with four different sizes: BERT-tiny (4.4M parameters), BERT-mini (11.3M parameters), BERT-small (29.1M parameters), and BERT-medium (41.7M parameters). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `transformer` library to train and evaluate all models. We train every model for 3 epochs with a batch size of 16, and evaluate every 50 steps. At the end of every training cycle, we evaluate the best checkpoint on the test data and remember the accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** prajjwal1/bert-tiny ***\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8915e7089214d179e1e1ca6ebe72fce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/285 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8713a57dc8c34be6a6c8c22445a14b2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04a4005ee91c40fdb37faf7aeb2b2063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/17.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/Users/jlheller/home/Technical/repos/nlp-notebooks/nlp/lib/python3.9/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62be86c1a39741199b5a2e17a748f2df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/17.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1689' max='1689' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1689/1689 01:06, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.349343</td>\n",
       "      <td>0.015984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.345959</td>\n",
       "      <td>0.015984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.339928</td>\n",
       "      <td>0.015984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.331118</td>\n",
       "      <td>0.015984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.320867</td>\n",
       "      <td>0.015984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.308610</td>\n",
       "      <td>0.015984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.288723</td>\n",
       "      <td>0.021978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.267305</td>\n",
       "      <td>0.018981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.238121</td>\n",
       "      <td>0.031968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.323700</td>\n",
       "      <td>4.210266</td>\n",
       "      <td>0.049950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>4.323700</td>\n",
       "      <td>4.165613</td>\n",
       "      <td>0.147852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>4.323700</td>\n",
       "      <td>4.110803</td>\n",
       "      <td>0.190809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>4.323700</td>\n",
       "      <td>4.064203</td>\n",
       "      <td>0.208791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>4.323700</td>\n",
       "      <td>4.012189</td>\n",
       "      <td>0.320679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>4.323700</td>\n",
       "      <td>3.959873</td>\n",
       "      <td>0.329670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>4.323700</td>\n",
       "      <td>3.917435</td>\n",
       "      <td>0.355644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>4.323700</td>\n",
       "      <td>3.873043</td>\n",
       "      <td>0.382617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>4.323700</td>\n",
       "      <td>3.828765</td>\n",
       "      <td>0.375624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>4.323700</td>\n",
       "      <td>3.790544</td>\n",
       "      <td>0.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>4.025800</td>\n",
       "      <td>3.756683</td>\n",
       "      <td>0.382617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>4.025800</td>\n",
       "      <td>3.718892</td>\n",
       "      <td>0.413586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>4.025800</td>\n",
       "      <td>3.685298</td>\n",
       "      <td>0.427572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>4.025800</td>\n",
       "      <td>3.657789</td>\n",
       "      <td>0.413586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>4.025800</td>\n",
       "      <td>3.631428</td>\n",
       "      <td>0.419580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>4.025800</td>\n",
       "      <td>3.606670</td>\n",
       "      <td>0.423576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>4.025800</td>\n",
       "      <td>3.584832</td>\n",
       "      <td>0.427572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>4.025800</td>\n",
       "      <td>3.565549</td>\n",
       "      <td>0.436563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>4.025800</td>\n",
       "      <td>3.550546</td>\n",
       "      <td>0.444555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>4.025800</td>\n",
       "      <td>3.537026</td>\n",
       "      <td>0.439560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.683100</td>\n",
       "      <td>3.526466</td>\n",
       "      <td>0.448551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>3.683100</td>\n",
       "      <td>3.516066</td>\n",
       "      <td>0.453546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>3.683100</td>\n",
       "      <td>3.509785</td>\n",
       "      <td>0.452547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>3.683100</td>\n",
       "      <td>3.506708</td>\n",
       "      <td>0.453546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='49' max='49' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [49/49 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** prajjwal1/bert-mini ***\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32dd8964246c467aac8fa36002c0766f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/286 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b7dc94cb65f4ec4a5fe948c66e9d169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f4a90928b75411aa6cd6f55745db453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/45.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-mini and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/Users/jlheller/home/Technical/repos/nlp-notebooks/nlp/lib/python3.9/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01a87a68ec104777818b54aab9c8ab64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/45.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1689' max='1689' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1689/1689 01:39, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.364165</td>\n",
       "      <td>0.011988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.350836</td>\n",
       "      <td>0.009990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.327166</td>\n",
       "      <td>0.014985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.290674</td>\n",
       "      <td>0.017982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.245788</td>\n",
       "      <td>0.025974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.169483</td>\n",
       "      <td>0.055944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.085116</td>\n",
       "      <td>0.078921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.994946</td>\n",
       "      <td>0.135864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.900339</td>\n",
       "      <td>0.221778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.205700</td>\n",
       "      <td>3.770319</td>\n",
       "      <td>0.283716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>4.205700</td>\n",
       "      <td>3.641401</td>\n",
       "      <td>0.298701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>4.205700</td>\n",
       "      <td>3.505606</td>\n",
       "      <td>0.377622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>4.205700</td>\n",
       "      <td>3.379586</td>\n",
       "      <td>0.400599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>4.205700</td>\n",
       "      <td>3.260160</td>\n",
       "      <td>0.441558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>4.205700</td>\n",
       "      <td>3.149359</td>\n",
       "      <td>0.462537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>4.205700</td>\n",
       "      <td>3.055441</td>\n",
       "      <td>0.474525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>4.205700</td>\n",
       "      <td>2.968087</td>\n",
       "      <td>0.483516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>4.205700</td>\n",
       "      <td>2.888928</td>\n",
       "      <td>0.505495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>4.205700</td>\n",
       "      <td>2.805550</td>\n",
       "      <td>0.529471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.281900</td>\n",
       "      <td>2.744091</td>\n",
       "      <td>0.532468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>3.281900</td>\n",
       "      <td>2.670273</td>\n",
       "      <td>0.568432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>3.281900</td>\n",
       "      <td>2.625335</td>\n",
       "      <td>0.568432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>3.281900</td>\n",
       "      <td>2.567434</td>\n",
       "      <td>0.583417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>3.281900</td>\n",
       "      <td>2.524451</td>\n",
       "      <td>0.575425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>3.281900</td>\n",
       "      <td>2.483152</td>\n",
       "      <td>0.583417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>3.281900</td>\n",
       "      <td>2.439162</td>\n",
       "      <td>0.609391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>3.281900</td>\n",
       "      <td>2.408895</td>\n",
       "      <td>0.607393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>3.281900</td>\n",
       "      <td>2.382743</td>\n",
       "      <td>0.625375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>3.281900</td>\n",
       "      <td>2.358209</td>\n",
       "      <td>0.616384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.580800</td>\n",
       "      <td>2.341320</td>\n",
       "      <td>0.622378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>2.580800</td>\n",
       "      <td>2.326037</td>\n",
       "      <td>0.622378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.580800</td>\n",
       "      <td>2.316873</td>\n",
       "      <td>0.624376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>2.580800</td>\n",
       "      <td>2.310877</td>\n",
       "      <td>0.622378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='49' max='49' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [49/49 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** prajjwal1/bert-small ***\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d4ea9e6a5914eda919de0d0edcad86f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/286 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e81eff99a1e64e2c8deb4b42a5aa0d6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0829c2517494b0c9d92c3effc650504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/116M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-small and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/Users/jlheller/home/Technical/repos/nlp-notebooks/nlp/lib/python3.9/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a833fc4e45aa4e59855a24b1ac902bf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/116M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1689' max='1689' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1689/1689 02:24, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.379958</td>\n",
       "      <td>0.017982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.334715</td>\n",
       "      <td>0.022977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.268477</td>\n",
       "      <td>0.035964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.147793</td>\n",
       "      <td>0.083916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.019597</td>\n",
       "      <td>0.118881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.763026</td>\n",
       "      <td>0.293706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.483508</td>\n",
       "      <td>0.384615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.209021</td>\n",
       "      <td>0.431568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.922090</td>\n",
       "      <td>0.523477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.875000</td>\n",
       "      <td>2.644598</td>\n",
       "      <td>0.563437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>3.875000</td>\n",
       "      <td>2.435997</td>\n",
       "      <td>0.557443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.875000</td>\n",
       "      <td>2.150161</td>\n",
       "      <td>0.666334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>3.875000</td>\n",
       "      <td>1.906593</td>\n",
       "      <td>0.691309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>3.875000</td>\n",
       "      <td>1.766210</td>\n",
       "      <td>0.705295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>3.875000</td>\n",
       "      <td>1.578361</td>\n",
       "      <td>0.743257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.875000</td>\n",
       "      <td>1.448539</td>\n",
       "      <td>0.770230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>3.875000</td>\n",
       "      <td>1.340675</td>\n",
       "      <td>0.789211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>3.875000</td>\n",
       "      <td>1.240151</td>\n",
       "      <td>0.808192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>3.875000</td>\n",
       "      <td>1.160519</td>\n",
       "      <td>0.821179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.820700</td>\n",
       "      <td>1.084700</td>\n",
       "      <td>0.828172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.820700</td>\n",
       "      <td>1.032535</td>\n",
       "      <td>0.831169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.820700</td>\n",
       "      <td>0.987895</td>\n",
       "      <td>0.836164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>1.820700</td>\n",
       "      <td>0.926638</td>\n",
       "      <td>0.864136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.820700</td>\n",
       "      <td>0.889429</td>\n",
       "      <td>0.855145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.820700</td>\n",
       "      <td>0.851847</td>\n",
       "      <td>0.865135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.820700</td>\n",
       "      <td>0.817091</td>\n",
       "      <td>0.860140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>1.820700</td>\n",
       "      <td>0.787062</td>\n",
       "      <td>0.872128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.820700</td>\n",
       "      <td>0.774760</td>\n",
       "      <td>0.866134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>1.820700</td>\n",
       "      <td>0.751587</td>\n",
       "      <td>0.869131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.926900</td>\n",
       "      <td>0.733683</td>\n",
       "      <td>0.876124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.926900</td>\n",
       "      <td>0.728317</td>\n",
       "      <td>0.878122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.926900</td>\n",
       "      <td>0.717968</td>\n",
       "      <td>0.873127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.926900</td>\n",
       "      <td>0.712835</td>\n",
       "      <td>0.879121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='49' max='49' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [49/49 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** prajjwal1/bert-medium ***\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90f4e1409b0e4c888a13828d2b2274c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/286 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52b264af12754c8ca0613ec2f0ede68d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eed945fc4d2486ab298aa19db294434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/167M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-medium and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/Users/jlheller/home/Technical/repos/nlp-notebooks/nlp/lib/python3.9/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1689' max='1689' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1689/1689 04:11, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.376102</td>\n",
       "      <td>0.014985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.319473</td>\n",
       "      <td>0.021978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.240968</td>\n",
       "      <td>0.064935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.067734</td>\n",
       "      <td>0.160839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.839127</td>\n",
       "      <td>0.223776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.536472</td>\n",
       "      <td>0.347652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.213123</td>\n",
       "      <td>0.436563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.898934</td>\n",
       "      <td>0.519481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.596041</td>\n",
       "      <td>0.601399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.703000</td>\n",
       "      <td>2.304779</td>\n",
       "      <td>0.637363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>3.703000</td>\n",
       "      <td>2.049459</td>\n",
       "      <td>0.679321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.703000</td>\n",
       "      <td>1.785054</td>\n",
       "      <td>0.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>3.703000</td>\n",
       "      <td>1.583426</td>\n",
       "      <td>0.757243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>3.703000</td>\n",
       "      <td>1.403178</td>\n",
       "      <td>0.755245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>3.703000</td>\n",
       "      <td>1.277946</td>\n",
       "      <td>0.805195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.703000</td>\n",
       "      <td>1.163509</td>\n",
       "      <td>0.822178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>3.703000</td>\n",
       "      <td>1.062698</td>\n",
       "      <td>0.831169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>3.703000</td>\n",
       "      <td>0.955967</td>\n",
       "      <td>0.852148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>3.703000</td>\n",
       "      <td>0.902524</td>\n",
       "      <td>0.847153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.509300</td>\n",
       "      <td>0.832187</td>\n",
       "      <td>0.869131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.509300</td>\n",
       "      <td>0.792724</td>\n",
       "      <td>0.860140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.509300</td>\n",
       "      <td>0.741002</td>\n",
       "      <td>0.871129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>1.509300</td>\n",
       "      <td>0.697332</td>\n",
       "      <td>0.878122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.509300</td>\n",
       "      <td>0.670368</td>\n",
       "      <td>0.879121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.509300</td>\n",
       "      <td>0.638547</td>\n",
       "      <td>0.884116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.509300</td>\n",
       "      <td>0.607420</td>\n",
       "      <td>0.884116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>1.509300</td>\n",
       "      <td>0.581806</td>\n",
       "      <td>0.895105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.509300</td>\n",
       "      <td>0.571743</td>\n",
       "      <td>0.889111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>1.509300</td>\n",
       "      <td>0.548433</td>\n",
       "      <td>0.896104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.666600</td>\n",
       "      <td>0.535165</td>\n",
       "      <td>0.900100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.666600</td>\n",
       "      <td>0.530965</td>\n",
       "      <td>0.901099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.666600</td>\n",
       "      <td>0.527799</td>\n",
       "      <td>0.899101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.666600</td>\n",
       "      <td>0.519731</td>\n",
       "      <td>0.906094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fdb95078f924ff58124fdf454a5901f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/167M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='49' max='49' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [49/49 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** albert-base-v2 ***\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee10faa6a8834bc6a85dd17dc4c59f7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13ac3c25d459412398e3ff60a6b0ec07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "415ce9b7adef410b92a96345abf096da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "488607c78456474ba853d0efda461761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "092faa97aeda4061beed002a37d569e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/47.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/jlheller/home/Technical/repos/nlp-notebooks/nlp/lib/python3.9/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1689' max='1689' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1689/1689 07:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.355213</td>\n",
       "      <td>0.021978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.309693</td>\n",
       "      <td>0.016983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.227697</td>\n",
       "      <td>0.040959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.048570</td>\n",
       "      <td>0.096903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.815527</td>\n",
       "      <td>0.133866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.421805</td>\n",
       "      <td>0.293706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.085482</td>\n",
       "      <td>0.340659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.724037</td>\n",
       "      <td>0.431568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.424218</td>\n",
       "      <td>0.499500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.565500</td>\n",
       "      <td>2.079331</td>\n",
       "      <td>0.580420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>3.565500</td>\n",
       "      <td>1.887030</td>\n",
       "      <td>0.594406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.565500</td>\n",
       "      <td>1.584998</td>\n",
       "      <td>0.692308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>3.565500</td>\n",
       "      <td>1.356243</td>\n",
       "      <td>0.751249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>3.565500</td>\n",
       "      <td>1.233593</td>\n",
       "      <td>0.755245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>3.565500</td>\n",
       "      <td>1.062890</td>\n",
       "      <td>0.798202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.565500</td>\n",
       "      <td>1.013945</td>\n",
       "      <td>0.802198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>3.565500</td>\n",
       "      <td>0.905890</td>\n",
       "      <td>0.809191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>3.565500</td>\n",
       "      <td>0.819412</td>\n",
       "      <td>0.831169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>3.565500</td>\n",
       "      <td>0.737925</td>\n",
       "      <td>0.843157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.205300</td>\n",
       "      <td>0.657943</td>\n",
       "      <td>0.868132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.205300</td>\n",
       "      <td>0.639231</td>\n",
       "      <td>0.867133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.205300</td>\n",
       "      <td>0.623232</td>\n",
       "      <td>0.872128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>1.205300</td>\n",
       "      <td>0.553036</td>\n",
       "      <td>0.876124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.205300</td>\n",
       "      <td>0.538011</td>\n",
       "      <td>0.879121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.205300</td>\n",
       "      <td>0.513787</td>\n",
       "      <td>0.893107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.205300</td>\n",
       "      <td>0.482704</td>\n",
       "      <td>0.883117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>1.205300</td>\n",
       "      <td>0.461757</td>\n",
       "      <td>0.897103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.205300</td>\n",
       "      <td>0.446824</td>\n",
       "      <td>0.895105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>1.205300</td>\n",
       "      <td>0.442660</td>\n",
       "      <td>0.902098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.449400</td>\n",
       "      <td>0.419786</td>\n",
       "      <td>0.896104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.449400</td>\n",
       "      <td>0.417248</td>\n",
       "      <td>0.899101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.449400</td>\n",
       "      <td>0.402718</td>\n",
       "      <td>0.902098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.449400</td>\n",
       "      <td>0.399261</td>\n",
       "      <td>0.907093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='49' max='49' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [49/49 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** albert-large-v2 ***\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e33100a9f457491cb5ae8141eb67fa71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fbd11ea9298405ba9b20e6600455a4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f2bf80dc16f4e1a828e64d9b218ad1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "117353b5444a4d3889f718b8cb959c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "963b1388e8fb4fac98b0157b551384e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/71.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-large-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/jlheller/home/Technical/repos/nlp-notebooks/nlp/lib/python3.9/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1689' max='1689' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1689/1689 21:28, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.338621</td>\n",
       "      <td>0.009990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.304392</td>\n",
       "      <td>0.024975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.232027</td>\n",
       "      <td>0.034965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.029878</td>\n",
       "      <td>0.110889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.683727</td>\n",
       "      <td>0.240759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.314065</td>\n",
       "      <td>0.341658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.922739</td>\n",
       "      <td>0.423576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.521385</td>\n",
       "      <td>0.507493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.142122</td>\n",
       "      <td>0.573427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.482900</td>\n",
       "      <td>1.794217</td>\n",
       "      <td>0.662338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>3.482900</td>\n",
       "      <td>1.609296</td>\n",
       "      <td>0.646354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.482900</td>\n",
       "      <td>1.317329</td>\n",
       "      <td>0.755245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>3.482900</td>\n",
       "      <td>1.124314</td>\n",
       "      <td>0.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>3.482900</td>\n",
       "      <td>1.001014</td>\n",
       "      <td>0.781219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>3.482900</td>\n",
       "      <td>0.888513</td>\n",
       "      <td>0.831169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.482900</td>\n",
       "      <td>0.808747</td>\n",
       "      <td>0.817183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>3.482900</td>\n",
       "      <td>0.721114</td>\n",
       "      <td>0.841159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>3.482900</td>\n",
       "      <td>0.659953</td>\n",
       "      <td>0.854146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>3.482900</td>\n",
       "      <td>0.608215</td>\n",
       "      <td>0.870130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.004700</td>\n",
       "      <td>0.553642</td>\n",
       "      <td>0.886114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.004700</td>\n",
       "      <td>0.592972</td>\n",
       "      <td>0.867133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.004700</td>\n",
       "      <td>0.522716</td>\n",
       "      <td>0.885115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>1.004700</td>\n",
       "      <td>0.456357</td>\n",
       "      <td>0.889111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.004700</td>\n",
       "      <td>0.440097</td>\n",
       "      <td>0.904096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.004700</td>\n",
       "      <td>0.440684</td>\n",
       "      <td>0.892108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.004700</td>\n",
       "      <td>0.402632</td>\n",
       "      <td>0.897103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>1.004700</td>\n",
       "      <td>0.411413</td>\n",
       "      <td>0.904096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.004700</td>\n",
       "      <td>0.387867</td>\n",
       "      <td>0.907093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>1.004700</td>\n",
       "      <td>0.357073</td>\n",
       "      <td>0.911089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.369900</td>\n",
       "      <td>0.350789</td>\n",
       "      <td>0.919081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.369900</td>\n",
       "      <td>0.350743</td>\n",
       "      <td>0.917083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.369900</td>\n",
       "      <td>0.333770</td>\n",
       "      <td>0.917083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.369900</td>\n",
       "      <td>0.331607</td>\n",
       "      <td>0.918082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='49' max='49' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [49/49 00:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** bert-base-uncased ***\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fa88b1cbb4c49dbb54dcac37e6f67e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c89339dba9514d679987ee45f6031f59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc87e3e75244492a85d99f2e244876bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "247c1e65a1a6409491ea47cd52413eb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "622cd957620e4fdba222f26dc09a79a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/jlheller/home/Technical/repos/nlp-notebooks/nlp/lib/python3.9/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1689' max='1689' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1689/1689 08:44, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.383289</td>\n",
       "      <td>0.008991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.352167</td>\n",
       "      <td>0.015984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.268270</td>\n",
       "      <td>0.036963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.120410</td>\n",
       "      <td>0.073926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.857863</td>\n",
       "      <td>0.169830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.526659</td>\n",
       "      <td>0.319680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.100811</td>\n",
       "      <td>0.489510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.657640</td>\n",
       "      <td>0.553447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.260931</td>\n",
       "      <td>0.624376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.627000</td>\n",
       "      <td>1.885876</td>\n",
       "      <td>0.701299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>3.627000</td>\n",
       "      <td>1.612291</td>\n",
       "      <td>0.734266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.627000</td>\n",
       "      <td>1.320260</td>\n",
       "      <td>0.780220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>3.627000</td>\n",
       "      <td>1.090127</td>\n",
       "      <td>0.812188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>3.627000</td>\n",
       "      <td>0.898544</td>\n",
       "      <td>0.841159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>3.627000</td>\n",
       "      <td>0.795828</td>\n",
       "      <td>0.869131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.627000</td>\n",
       "      <td>0.714850</td>\n",
       "      <td>0.864136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>3.627000</td>\n",
       "      <td>0.656664</td>\n",
       "      <td>0.872128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>3.627000</td>\n",
       "      <td>0.579089</td>\n",
       "      <td>0.880120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>3.627000</td>\n",
       "      <td>0.555771</td>\n",
       "      <td>0.879121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.001200</td>\n",
       "      <td>0.486134</td>\n",
       "      <td>0.906094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.001200</td>\n",
       "      <td>0.468838</td>\n",
       "      <td>0.900100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.001200</td>\n",
       "      <td>0.443959</td>\n",
       "      <td>0.908092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>1.001200</td>\n",
       "      <td>0.398386</td>\n",
       "      <td>0.917083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.001200</td>\n",
       "      <td>0.401319</td>\n",
       "      <td>0.910090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.001200</td>\n",
       "      <td>0.367436</td>\n",
       "      <td>0.919081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.001200</td>\n",
       "      <td>0.349376</td>\n",
       "      <td>0.925075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>1.001200</td>\n",
       "      <td>0.360996</td>\n",
       "      <td>0.917083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.001200</td>\n",
       "      <td>0.359339</td>\n",
       "      <td>0.917083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>1.001200</td>\n",
       "      <td>0.328795</td>\n",
       "      <td>0.930070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.312000</td>\n",
       "      <td>0.323674</td>\n",
       "      <td>0.930070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.312000</td>\n",
       "      <td>0.324693</td>\n",
       "      <td>0.930070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.312000</td>\n",
       "      <td>0.321046</td>\n",
       "      <td>0.930070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.312000</td>\n",
       "      <td>0.319041</td>\n",
       "      <td>0.933067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/02/lb248q1j50dch2pthzkvxmpr0000gn/T/ipykernel_55934/48522100.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='49' max='49' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [49/49 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, Trainer, TrainingArguments, AutoModelForSequenceClassification\n",
    "\n",
    "model_ids = [\"prajjwal1/bert-tiny\", \"prajjwal1/bert-mini\", \n",
    "             \"prajjwal1/bert-small\", \"prajjwal1/bert-medium\",\n",
    "             \"albert-base-v2\", \"albert-large-v2\", \"bert-base-uncased\"]\n",
    "\n",
    "accuracies = []\n",
    "for model_id in model_ids:\n",
    "    \n",
    "    print(f\"*** {model_id} ***\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=len(label_names))\n",
    "\n",
    "    train_texts_encoded = tokenizer(train_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    dev_texts_encoded = tokenizer(dev_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    test_texts_encoded = tokenizer(test_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    \n",
    "    train_dataset = ClassificationDataset(train_texts_encoded, train_labels)\n",
    "    dev_dataset = ClassificationDataset(dev_texts_encoded, dev_labels)\n",
    "    test_dataset = ClassificationDataset(test_texts_encoded, test_labels)\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=64,\n",
    "        warmup_steps=int(len(train_dataset)/16),\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=50,\n",
    "        save_steps=50,\n",
    "        save_total_limit=10,\n",
    "        load_best_model_at_end=True,\n",
    "        no_cuda=False\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=dev_dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    test_results = trainer.evaluate(test_dataset)\n",
    "    \n",
    "    accuracies.append(test_results[\"eval_accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the results. It's no surprise that BERT-base-uncased emerges as the best model: larger models have some clear advantages to their smaller competitors. At the other end of the spectrum, BERT-tiny and BERT-mini are definitely too small for this task. Still, the four models in the middle prove fairly strong competitors to BERT. The accuracy of BERT-small is around 6% lower than that of BERT, but BERT-medium is less than 3% behind. ALBERT is even closer, with ALBERT-base landing at less than 1.5% below the accuracy of the much larger BERT. In environments where there is not sufficient memory to run a full BERT, ALBERT-base may prove a very effictive solution indeed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          model\n",
      "prajjwal1/bert-tiny    0.408117\n",
      "prajjwal1/bert-mini    0.598052\n",
      "prajjwal1/bert-small   0.862987\n",
      "prajjwal1/bert-medium  0.895779\n",
      "albert-base-v2         0.910390\n",
      "albert-large-v2        0.916234\n",
      "bert-base-uncased      0.927273\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x178bff160>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzoAAAH5CAYAAABJUkuHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8ekN5oAAAACXBIWXMAAA9hAAAPYQGoP6dpAABPXUlEQVR4nO3dB3RVVfr+8TeU0IuI9N5RqnQsgNIsIJYRkT/FAjqKA1IEBCmiYKMooChKGRswoyAo4igIiqAIiIJSBGmiNEV6h/969vzOnZtwE5KQkLD5fta6Kzn33lPjrDkP77v3iTp9+vRpAwAAAACPpEvtAwAAAACA5EbQAQAAAOAdgg4AAAAA7xB0AAAAAHiHoAMAAADAOwQdAAAAAN4h6AAAAADwTga7AJw6dcp+++03y5Ejh0VFRaX24QAAAABIJXoM6P79+61QoUKWLl26CzvoKOQULVo0tQ8DAAAAQBqxdetWK1KkyIUddFTJCU4mZ86cqX04AAAAAFLJvn37XBEkyAgXdNAJ2tUUcgg6AAAAAKLOMqSFyQgAAAAAeIegAwAAAMA7BB0AAAAA3iHoAAAAAPAOQQcAAACAdwg6AAAAALxD0AEAAADgHYIOAAAAAO8QdAAAAAB4h6ADAAAAwDsEHQAAAADeIegAAAAA8A5BBwAAAIB3CDoAAAAAvEPQAQAAAOAdgg4AAAAA7xB0AAAAAHiHoAMAAADAOwQdAAAAAN7JkNoHAAAAkNx+7fOl+arIM9ek9iEAFwQqOgAAAAC8Q9ABAAAA4B2CDgAAAADvEHQAAAAAeIegAwAAAMA7zLoGAMBFaHjrm81nrUv2Tu1DAJDKqOgAAAAA8A4VHQAA4jD2wXmpfQgAgCSiogMAAADAOwQdAAAAAN4h6AAAAADwDkEHAAAAgHcIOgAAAAC8Q9ABAAAA4B2mlwYAnJPVFSqatxqOTe0jAAAkERUdAAAAAN4h6AAAAADwDkEHAAAAgHcIOgAAAAC8Q9ABAAAA4B2CDgAAAADvEHQAAAAAeIegAwAAAMA7BB0AAAAA3iHoAAAAAPAOQQcAAACAdwg6AAAAALxD0AEAAADgHYIOAAAAAO8QdAAAAAB4J0NqHwAA+K7y5Mrms2mpfQAAAERARQcAAACAdwg6AAAAALxD0AEAAADgHYIOAAAAAO8QdAAAAAB4h6ADAAAAwDsEHQAAAADeIegAAAAA8A5BBwAAAIB3CDoAAAAAvEPQAQAAAOAdgg4AAAAA7xB0AAAAAHiHoAMAAADAOwQdAAAAAN4h6AAAAADwDkEHAAAAgHcIOgAAAAC8Q9ABAAAA4B2CDgAAAADvEHQAAAAAeIegAwAAAMA7BB0AAAAA3iHoAAAAAPAOQQcAAACAd5IUdMaOHWslSpSwzJkzW506dWzJkiXxfn/UqFFWvnx5y5IlixUtWtQeffRRO3LkSFKPGQAAAACSN+hMnTrVunfvbgMHDrTly5db1apVrVmzZrZz586I33/nnXesT58+7vurV6+2N954w23j8ccfT+yuAQAAACBlgs6IESOsU6dOds8999jll19u48aNs6xZs9qECRMifn/RokV21VVX2d133+2qQE2bNrU2bdqctQoEAAAAAOcl6Bw7dsyWLVtmjRs3/t8G0qVzy4sXL464Tv369d06QbD55ZdfbPbs2XbjjTfGuZ+jR4/avn37YrwAAAAAIKEyJPibZrZ79247efKk5c+fP8b7Wl6zZk3EdVTJ0XpXX321nT592k6cOGEPPvhgvK1rw4YNs8GDByfm0AAAAADg/M26Nn/+fBs6dKi9/PLLbkzP+++/bx999JENGTIkznX69u1re/fuDb22bt2a0ocJAAAA4GKt6OTNm9fSp09vO3bsiPG+lgsUKBBxnSeeeMLatWtn999/v1uuXLmyHTx40Dp37mz9+vVzrW+xZcqUyb0AAAAAIMUrOtHR0VajRg2bO3du6L1Tp0655Xr16kVc59ChQ2eEGYUlUSsbAAAAAKRqRUc0tXSHDh2sZs2aVrt2bfeMHFVoNAubtG/f3goXLuzG2UiLFi3cTG3Vq1d3z9xZv369q/Lo/SDwAAAAAECqBp3WrVvbrl27bMCAAbZ9+3arVq2azZkzJzRBwZYtW2JUcPr3729RUVHu57Zt2+yyyy5zIefpp59O1hMBAAAAgEDU6Qugf0zTS+fKlctNTJAzZ87UPhwASJTKkyubz6YNO2G+mtdwrPnqyJ4R5rPWJXubr4o8c01qHwJwQWSDFJ91DQAAAADON4IOAAAAAO8QdAAAAAB4h6ADAAAAwDsEHQAAAADeIegAAAAA8A5BBwAAAIB3CDoAAAAAvEPQAQAAAOAdgg4AAAAA7xB0AAAAAHiHoAMAAADAOwQdAAAAAN4h6AAAAADwDkEHAAAAgHcIOgAAAAC8Q9ABAAAA4B2CDgAAAADvEHQAAAAAeIegAwAAAMA7BB0AAAAA3iHoAAAAAPAOQQcAAACAdwg6AAAAALxD0AEAAADgHYIOAAAAAO8QdAAAAAB4h6ADAAAAwDsEHQAAAADeIegAAAAA8A5BBwAAAIB3CDoAAAAAvEPQAQAAAOAdgg4AAAAA7xB0AAAAAHiHoAMAAADAOwQdAAAAAN4h6AAAAADwDkEHAAAAgHcIOgAAAAC8Q9ABAAAA4B2CDgAAAADvEHQAAAAAeIegAwAAAMA7BB0AAAAA3iHoAAAAAPAOQQcAAACAdwg6AAAAALxD0AEAAADgHYIOAAAAAO8QdAAAAAB4h6ADAAAAwDsEHQAAAADeIegAAAAA8A5BBwAAAIB3CDoAAAAAvEPQAQAAAOAdgg4AAAAA7xB0AAAAAHiHoAMAAADAOwQdAAAAAN4h6AAAAADwDkEHAAAAgHcIOgAAAAC8Q9ABAAAA4B2CDgAAAADvEHQAAAAAeIegAwAAAMA7BB0AAAAA3iHoAAAAAPAOQQcAAACAdwg6AAAAALxD0AEAAADgHYIOAAAAAO8QdAAAAAB4h6ADAAAAwDsEHQAAAADeIegAAAAA8A5BBwAAAIB3CDoAAAAAvEPQAQAAAOAdgg4AAAAA7xB0AAAAAHiHoAMAAADAO0kKOmPHjrUSJUpY5syZrU6dOrZkyZJ4v//XX3/Zww8/bAULFrRMmTJZuXLlbPbs2Uk9ZgAAAACIVwZLpKlTp1r37t1t3LhxLuSMGjXKmjVrZmvXrrV8+fKd8f1jx45ZkyZN3Gf//ve/rXDhwrZ582bLnTt3YncNAAAAACkTdEaMGGGdOnWye+65xy0r8Hz00Uc2YcIE69Onzxnf1/t//vmnLVq0yDJmzOjeUzUIAAAAANJE65qqM8uWLbPGjRv/bwPp0rnlxYsXR1xn5syZVq9ePde6lj9/fqtUqZINHTrUTp48Ged+jh49avv27YvxAgAAAIAUqejs3r3bBRQFlnBaXrNmTcR1fvnlF5s3b561bdvWjctZv369PfTQQ3b8+HEbOHBgxHWGDRtmgwcPTsyhAbjQDcpl3ipZLLWPAACAi06Kz7p26tQpNz7ntddesxo1aljr1q2tX79+ruUtLn379rW9e/eGXlu3bk3pwwQAAABwsVZ08ubNa+nTp7cdO3bEeF/LBQoUiLiOZlrT2BytF6hYsaJt377dtcJFR0efsY5mZtMLAAAAAFK8oqNQoqrM3LlzY1RstKxxOJFcddVVrl1N3wusW7fOBaBIIQcAAAAAznvrmqaWHj9+vE2ePNlWr15tf//73+3gwYOhWdjat2/vWs8C+lyzrnXt2tUFHM3QpskINDkBAAAAAKSJ6aU1xmbXrl02YMAA135WrVo1mzNnTmiCgi1btriZ2AJFixa1Tz75xB599FGrUqWKe46OQk/v3r2T90wAAAAAIKlBR7p06eJekcyfP/+M99TW9vXXXydlVwAAAACQ9mZdAwAAAIDzjaADAAAAwDsEHQAAAADeIegAAAAA8A5BBwAAAIB3CDoAAAAAvEPQAQAAAOAdgg4AAAAA7xB0AAAAAHiHoAMAAADAOwQdAAAAAN4h6AAAAADwDkEHAAAAgHcIOgAAAAC8Q9ABAAAA4B2CDgAAAADvEHQAAAAAeIegAwAAAMA7BB0AAAAA3iHoAAAAAPAOQQcAAACAdwg6AAAAALxD0AEAAADgHYIOAAAAAO8QdAAAAAB4h6ADAAAAwDsEHQAAAADeIegAAAAA8A5BBwAAAIB3CDoAAAAAvEPQAQAAAOAdgg4AAAAA7xB0AAAAAHiHoAMAAADAOwQdAAAAAN4h6AAAAADwDkEHAAAAgHcIOgAAAAC8Q9ABAAAA4B2CDgAAAADvEHQAAAAAeIegAwAAAMA7BB0AAAAA3iHoAAAAAPAOQQcAAACAdwg6AAAAALxD0AEAAADgHYIOAAAAAO8QdAAAAAB4h6ADAAAAwDsEHQAAAADeIegAAAAA8A5BBwAAAIB3CDoAAAAAvEPQAQAAAOAdgg4AAAAA7xB0AAAAAHiHoAMAAADAOwQdAAAAAN4h6AAAAADwDkEHAAAAgHcIOgAAAAC8Q9ABAAAA4B2CDgAAAADvEHQAAAAAeIegAwAAAMA7BB0AAAAA3iHoAAAAAPAOQQcAAACAdwg6AAAAALxD0AEAAADgHYIOAAAAAO8QdAAAAAB4h6ADAAAAwDsEHQAAAADeIegAAAAA8A5BBwAAAIB3CDoAAAAAvEPQAQAAAOAdgg4AAAAA7xB0AAAAAHiHoAMAAADAOwQdAAAAAN4h6AAAAADwToakrDR27Fh7/vnnbfv27Va1alUbPXq01a5d+6zrTZkyxdq0aWO33HKLzZgxIym7Bi5aJfp8ZD7blDm1jwAAAFzUFZ2pU6da9+7dbeDAgbZ8+XIXdJo1a2Y7d+6Md71NmzZZz5497ZprrjmX4wUAAACA5A86I0aMsE6dOtk999xjl19+uY0bN86yZs1qEyZMiHOdkydPWtu2bW3w4MFWqlSps+7j6NGjtm/fvhgvAAAAAEiRoHPs2DFbtmyZNW7c+H8bSJfOLS9evDjO9Z588knLly+f3XfffQnaz7BhwyxXrlyhV9GiRRNzmAAAAAAucokKOrt373bVmfz588d4X8sarxPJwoUL7Y033rDx48cneD99+/a1vXv3hl5bt25NzGECAAAAuMglaTKChNq/f7+1a9fOhZy8efMmeL1MmTK5FwAAAACkeNBRWEmfPr3t2LEjxvtaLlCgwBnf37Bhg5uEoEWLFqH3Tp069d8dZ8hga9eutdKlSyfpwAEAAAAgWVrXoqOjrUaNGjZ37twYwUXL9erVO+P7FSpUsJUrV9qKFStCr5YtW1qjRo3c74y9AQAAAJAmWtc0tXSHDh2sZs2a7tk5o0aNsoMHD7pZ2KR9+/ZWuHBhN6FA5syZrVKlSjHWz507t/sZ+30AAAAASLWg07p1a9u1a5cNGDDATUBQrVo1mzNnTmiCgi1btriZ2AAAAADggpqMoEuXLu4Vyfz58+Ndd9KkSUnZJQAAAAAkGKUXAAAAAN4h6AAAAADwDkEHAAAAgHcIOgAAAAC8Q9ABAAAA4B2CDgAAAADvEHQAAAAAeIegAwAAAMA7BB0AAAAA3iHoAAAAAPAOQQcAAACAdwg6AAAAALxD0AEAAADgHYIOAAAAAO8QdAAAAAB4h6ADAAAAwDsEHQAAAADeIegAAAAA8A5BBwAAAIB3CDoAAAAAvEPQAQAAAOAdgg4AAAAA7xB0AAAAAHiHoAMAAADAOxlS+wAAAAB8dSq92aksUWZRybfNI0eOJN/GgDQoffr0liFDBouKOrf/4RB0AAAAUsCxPOnsYM3MZpmTt4Hm8MaNybo9IC3KmjWrFSxY0KKjo5O8DYIOAABAClRyFHKy589ll2bNbXaO/zIdLrpAtmTbFpDWnD592o4dO2a7du2yjRs3WtmyZS1duqT9YwFBBwAAIJm5drXM6VzIyZwxU7JuOzpz5mTdHpDWZMmSxTJmzGibN292oSdzEv+bZzICAACA5BYUcJKxkgNcTNIlsYoTYxvJciQAAAAAkIYQdAAAAAB4hzE6AAAA50m5MV+c1/1teuYmSwsmTZpk3bp1s7/++ivicnIbNGiQzZgxw1asWJHkbWzatMlKlixp3333nVWrVs3SkrR8bGkJFR0AAACkqNatW9u6deviXE5pP/74o91+++1WokQJ92yWUaNGxfnde+65x/r375/ix6TjUBg7m44dO1qrVq1ivFe0aFH7/fffrVKlSil4hBc+gg4AAACSRDNiJXQWrXz58sW5nNIOHTpkpUqVsmeeecYKFCgQ5/dOnjxpH374obVs2TLVr9nZHqip89BDNRE3gg4AAACchg0bWpcuXdwrV65cljdvXnviiSfcs01EFZEhQ4ZY+/btLWfOnNa5c2f3fu/eva1cuXLuIY8KFFrn+PHjoe2qVS137twRl/fu3etu3JcuXeqWT506ZXny5LG6deuGvv/WW2+5KkbgbPuLrVatWvb888/bXXfdZZkyxT3d96JFi9y0xvp+YM2aNVa/fn03xbEqKAsWLIixzqpVq+yGG26w7NmzW/78+a1du3a2e/fuM66pWvV0PZs1a+auo9x6662ushMsR2rBmzx5sn3wwQfue3rNnz/fta7p96A1T+9pee7cuVazZk13XerXr29r1651n+v7msUsuMYBVbaKFy/urrmPCDoAAAAI0Y21KgVLliyxF1980UaMGGGvv/566PMXXnjBqlat6saHKGBIjhw5XHj56aef3Drjx4+3kSNHJmh/ClQaZ6KbdVm5cqW7adf2Dxw44N5TuGjQoEFonXPZX3xmzpxpLVq0cPsP9OrVy3r06OGOp169eu7zP/74w32mMUbXXXedVa9e3YWIOXPm2I4dO+zOO+8845pGR0fbV199ZePGjbNvv/3WvT9x4kTXghYsx9azZ0+3rebNm7vv6aUAE5d+/frZ8OHD3bFkyJDB7r33Xve+glTjxo3d/sJpWa1xyTGVc1rk51kBAAAgSVQ5UWgoX768tW3b1h555JEYIUI39rrxL126tHuJxrToBlw31AoCukGfNm1agvepqkcQdPSzSZMmVrFiRVu4cGHovfCgc677i4sqJ7Hb1lSN0fgeHc8rr7zigtkbb7zhPhszZowLOUOHDrUKFSq43ydMmGCff/55jDFIZcuWteeee85dU70uu+wy976qWmpBC5ZjU5VIbX6qQul7eikwxeXpp5921+nyyy+3Pn36uArVkSNH3Gf333+/vfvuu3b06FG3vHz5chcqNSbJVwQdAAAAhKhlLLyioSrGzz//7MaviFqjYps6dapdddVV7kZcN+cKIlu2bEnwPnVzrlCjfah6o+AThJ/ffvvN1q9f75aTa3+RrF692u3r+uuvj/G+zj+gKonOX9+V77//3oUaHUPwUuCRDRs2hNarUaPGWfev4w/fjsJTYlWpUiX0e8GCBd3PnTt3up+a0EAtgtOnT3fLqog1atQozrY5HzCCCQAAAAmWLVu2GMuLFy92lZ/Bgwe78SeqeEyZMsW1UCXUtddea/v373dVhi+++MLd5CvEaPIAtckVKlTIVUWSa39xta2pkqSxOAml1jpVlJ599tkzPguCRqRrFonOMXw6bI1TSiyNLwpE/V9YDcbfqBKksVVqV7vtttvsnXfecW1/PiPoAAAAIOSbb76Jsfz111+7kKFqQCRqj9KAdo0PCWzevDlR+1QLl6oRagXTzbqqIpqVTdNQaxa08La15NhfXG1rweQKsc9fQUxOnDhhy5Ytc+1scuWVV9p7773nqiKJnQFN5xlUyUTrlylT5ozvKaCEf+9c3H///W5ChZdfftmdiwKPz2hdAwAAQIwWqu7du7sZuzSmY/To0da1a9c4v68QpHVUVVG71ksvvRRqj0oMtaa9/fbboVCjiobGxahNLTzoJGV/mtJZ1RK99Pu2bdvc72qJC9q7NID/5ptvPmPdsWPHuu1r9rWHH37Y9uzZExrkr+U///zT2rRp4yYU0PF88sknbtzL2cKJwpFmSdu+fbvbZnzf++GHH9zfQ7O5xTe73NlUrFjRtSZq1jods8b/+IyKDgAAwHmyrst/KwPnIrpIDktJam86fPiw1a5d21VxFHIiVToCGrz/6KOPuiqHBrrfdNNNbjY2TY2cGAozmu44fCyOftc4mPD3krI/jb3RRAHhM8fppX1qHNCsWbPc+Wr659jUPqeXgpEqLmpxC76ndjPNpKbg0LRpU3c8qjZplrSzzWSmVjsFSs0YV7hwYTcFdCSdOnVyx6ixQWqV05igcxlXc99997mqWBDWfBZ1OpgYPQ3bt2+f67/UPOuasx24GJXo85H5bFPmu81XlUsWM59NG3bCfDWv4Vjz1ZE9I8xnrUv2TtX9n8gRZfsbZbPihYpZ5gxxz5KV1oKOAoWmelbgSE6vvvqqe/7Or7/+GnE5tSk8XX311fbYY4+Z74YMGWL/+te/XJUoLdNscRs3brSSJUueMW4qodmA1jUAAACkmK1bt9rs2bPtiiuuiLicFijkqJXLZwcOHHAPN9U4KE0ZfjGgdQ0AAAApRgP21Zql6YwjLacFF0Mlp0uXLm7MlaaZvhja1oSgAwAAACd4aGdy2rVrV7zLOD8mTZqUpsLl+UDrGgAAAADvEHQAAAAAeIegAwAAAMA7BB0AAAAA3iHoAAAAAPAOQQcAAACAd5heGgAA4DyJfr3I+d3hoL2WFmha427dutlff/0VcTm5DRo0yGbMmGErVqxI8jY2bdpkJUuWtO+++86qVatmF5KkHPugZLhmaQ0VHQAAAKSo1q1b27p16+JcTmk//vij3X777VaiRAmLioqyUaNGxfnde+65x/r375/ix6TjULBICUWLFrXff//dKlWqlOB1evbsaXPnzjWfEHQAAACQJMeOHUvQ97JkyWL58uWLczmlHTp0yEqVKmXPPPOMFShQIM7vnTx50j788ENr2bJlql+zc5E+fXp3nhkyJLx5K3v27HbppZeaTwg6AAAAcBo2bGhdunRxr1y5clnevHntiSeesNOnT7vPVREZMmSItW/f3nLmzGmdO3d27/fu3dvKlStnWbNmdYFC6xw/fjy0XbWq5c6dO+Ly3r173Y350qVL3fKpU6csT548Vrdu3dD333rrLVelCJxtf7HVqlXLnn/+ebvrrrssU6ZMcX5v0aJFljFjRvf9wJo1a6x+/fqWOXNmVyFZsGBBjHVWrVplN9xwgwsK+fPnt3bt2tnu3bvPuKZq1dP1bNasmbuOcuutt7rKTrAcSceOHa1Vq1Y2dOhQt31dtyeffNJOnDhhvXr1cteqSJEiNnHixBita1FRUaE2tPnz57tlVWxq1qzprpvOae3atTFa1y60Fr2zIegAAAAgZPLkya4SsGTJEnvxxRdtxIgR9vrrr4c+f+GFF6xq1apu/IcChuTIkcOFl59++smtM378eBs5cmSC9qdApRts3YzLypUr3U25tn/gwAH3nsJFgwYNQuucy/7iM3PmTGvRooXbf0BhokePHu546tWr5z7/448/3GcaY3TddddZ9erVXVCbM2eO7dixw+68884zrml0dLR99dVXNm7cOPv222/d+wonajELluMyb948++233+yLL75wf4+BAwfazTffbJdccol988039uCDD9oDDzxgv/76a7zb6devnw0fPtwdq/7G9957r/mMoAMAAIAQVU4UGsqXL29t27a1Rx55JEaI0I29bvxLly7tXqIxLaoQqDKhIKDxHtOmTUvwPlX1CIKOfjZp0sQqVqxoCxcuDL0XHnTOdX9x+eCDD85oW1M1RuN7dDyvvPKKC2ZvvPGG+2zMmDEu5KjaUqFCBff7hAkT7PPPP48xBqls2bL23HPPuWuq12WXXebeV3VGLWbBclxUtXnppZfcugon+ql2vMcff9xtu2/fvi5IBdcrLk8//bS7jpdffrn16dPHVbCOHDlyDlcsbSPoAAAAIEQtY+EVDVUxfv75Zzd+RdT6FNvUqVPtqquucjftauFSENmyZUuC96mbb92kax+q3ij4BOFHlYz169e75eTaXySrV692+7r++utjvK/zD6gKovPXd+X77793oUbHELwUeGTDhg2h9WrUqHHW/ev4w7ej8BS44oorLF26/922q4WtcuXKoWW1/ml8zc6dO+PdR5UqVUK/FyxY0P082zoXMqaXBgAAQIJly5YtxvLixYtd5Wfw4MFu/IkqHlOmTHEtUgl17bXX2v79+2358uWuPUs3+QoxmjxAbXKFChVylYvk2l9cbWuqJGksTkKptU4VpWefffaMz4IgEemaRaJzDJ/aWVWcgMYNhVMQjfSexjfFJ2PYOkGYPds6FzKCDgAAAEI05iPc119/7UKGqgaRqP2pePHibvxHYPPmzYnap1q4VG1QK5huxlUV0axsmoZas6CFt60lx/7ialsLJleIff4KYqIJAJYtW+ba2eTKK6+09957z7XQJWaGM9F5BlUy0fplypQ55/PA/9C6BgAAgBgtVN27d3czcr377rs2evRo69q1a5zfVwjSOqqqqF1LY0mmT5+e6P2qNe3tt98OhRpVNDQuRm1q4UEnKfvTlM6qluil37dt2+Z+V0tc0L6lAfoa4B/b2LFj3fY1+9rDDz9se/bsCQ3i1/Kff/5pbdq0cRMK6Hg++eQT9yye8BATicKRZkHbvn272yaSHxUdAACA8+TY/fHPipUQ0UVyWErS1NGHDx+22rVruyqOQk6kSkdAg/cfffRRV+U4evSo3XTTTW42Nk1XnBgKM3qQZ/hYHP2ucTDh7yVlfxp7o4kCwmeO00v71DigWbNmufPV9M+xqX1OLwUjVVzU4hZ8T+1mmklN0103bdrUHY+qTc2bN48xpiYStdopUGrGuMKFC7spoZG8ok4HE6OnYfv27XP9l5pnXXO2AxejEn0+Mp9tyny3+apyyWLms2nDTpiv5jUca746smeE+ax1yd6puv8TOaJsf6NsVrxQMcucITpZt52SQUeBQlM9K3Akp1dffdU9fyeY/jj2cmpTeLr66qvtscceS+1Dwf/RbHAbN260kiVLnjFuKqHZgNY1AAAApJitW7fa7Nmz3cxhkZbTAoUctZ/BL7SuAQAAIMVowL5as/SAz0jLaQGVHD8RdAAAAOAED+1MTrt27Yp3GUgptK4BAAAA8A5BBwAAAIB3CDoAAAAAvEPQAQAAAOAdgg4AAAAA7xB0AAAAAHiH6aUBAADOkxpz65/X/a3ssNLSAj0zp1u3bvbXX39FXE5ugwYNshkzZtiKFSuSvI1NmzZZyZIl7bvvvrNq1arZxWRSrL9PclzP1EBFBwAAACmqdevWtm7dujiXU9qPP/5ot99+u5UoUcKioqJs1KhRcX73nnvusf79+6f4Mek4FB6Qcgg6AAAASJJjx44l6HtZsmSxfPnyxbmc0g4dOmSlSpWyZ555xgoUKBDn906ePGkffvihtWzZMtWvGc4dQQcAAABOw4YNrUuXLu6VK1cuy5s3rz3xxBN2+vRp97kqIkOGDLH27dtbzpw5rXPnzu793r17W7ly5Sxr1qwuUGid48ePx2iFyp07d8TlvXv3Wvr06W3p0qVu+dSpU5YnTx6rW7du6PtvvfWWFS1aNLR8tv3FVqtWLXv++eftrrvuskyZMsX5vUWLFlnGjBnd9wNr1qyx+vXrW+bMma1SpUq2YMGCGOusWrXKbrjhBsuePbvlz5/f2rVrZ7t37z7jmqoVTNezWbNm7jrKrbfe6io7wXIk8+fPt9q1a1u2bNncNbvqqqts8+bNoZYytdVNmDDBihUr5o7hoYcecoHtueeec6FOgfLpp5+Osc0RI0ZY5cqV3TZ1XbXOgQMHzDcEHQAAAIRMnjzZMmTIYEuWLLEXX3zR3RS//vrroc9feOEFq1q1qhu7ooAhOXLkcOHlp59+cuuMHz/eRo4cmaD9KVDpZl039LJy5Up386/tBzffChcNGjQIrXMu+4vPzJkzrUWLFm7/gV69elmPHj3c8dSrV899/scff7jPNIbluuuus+rVq7ugNmfOHNuxY4fdeeedZ1zT6Oho++qrr2zcuHH27bffuvcnTpxov//+e2g5thMnTlirVq3cuf/www+2ePFiFy7Dj2/Dhg328ccfu32/++679sYbb9hNN91kv/76q7tuzz77rGvF++abb0LrpEuXzl566SXX0qdjmzdvnj322GPmGyYjAAAAQIj+hV+hQTfT5cuXd8FDy506dXKf68ZeN/7hwse0qDrRs2dPmzJlSoJvnlX1UNDRevrZpEkTV0lZuHChNW/e3L0Xvq1z3V9cPvjggzMCk6oxGt8jr7zyigsUChPa15gxY1zIGTp0aOj7qq7oGmoMkqpOUrZsWVdhiU0Vmvha6fbt2+cqXjfffLOVLl3avVexYsUY31EFTPtU+Lv88sutUaNGtnbtWps9e7YLNPobKux8/vnnVqdOHbeOqkvh1++pp56yBx980F5++WXzCRUdAAAAhKhlLLxioCrGzz//7NqhpGbNmmesM3XqVNdSpZt2tU8piGzZsiXB+1TFQqFG+1AVQsEnCD+//fabrV+/3i0n1/4iWb16tdvX9ddfH+N9nX9AlS6dv74r33//vQsQOobgVaFChVClJVCjRo2z7l/HH74dhSe18HXs2NG1u6mSpOqVKkDhFFQUcgJqn1PgSZcuXYz3du7cGVr+7LPP3HkWLlzYrat2O1WpNJbJJwQdAAAAJJjGdYRTO1Xbtm3txhtvdAP51eLVr1+/RA26v/baa23//v22fPly++KLL2IEHQWfQoUKuapIcu0vrrY1VZI0Fieh1FqnAKJpl8NfCoY6p7iuWSQ6x/BtqMIStLfpnDVOSAFPVaKvv/46tJ7GFIVTSI303qlTp0LTZqtCVKVKFXvvvfds2bJlNnbsWC8nSqB1DQAAACHhYzlEN9UKGZowIK4B/MWLF3dhIxAMlk8otXDpxlutYLpJV1VEg+g1DbXCTPj4nOTYX1xta8HkCrHPPwgtGjOjYKB2NrnyyitdWFBVRdWexNB5BlUy0fplypSJ+F21x+nVt29fV2F65513YkzWkBjLli1zoWf48OGhqs+0adPMR1R0AAAAEKOFqnv37m6chwa3jx492rp27Rrn9xWCtI7GyKhdS4Pcp0+fnuj9qoLz9ttvh0KN2rY0HkVVjPCgk5T9qVIRVEr0+7Zt29zvaokTtXVpMgFVOmJTtUPb15ihhx9+2Pbs2WP33nuv+0zLf/75p7Vp08ZNKKDj+eSTT9yzeMJDTCQKR3PnzrXt27e7bUayceNGF25U0VGY+89//uOqRbHH6SRGmTJl3Ax1+rv+8ssv9uabb7oJEnxERQcAAOA8WXb9onPeRnSR/43HSAmaOvrw4cNuSmNVcRRyIlU6AnrmzKOPPuqqHEePHnUzfmk2Nk19nBgKM3qQZ/hYHP2ucTDh7yVlfxp7o4pI+Mxxemmfao+bNWuWO19N/xybnr2jl4KRQoJa3ILvqd1MM6lpuuumTZu641G1SRMohI+RiUQVFQVKzRinsTJqKYtN02crYGlmNI2hKViwoAtXDzzwgCVV1apV3Ux6mqBAIUrVqmHDhrm/u2+iTgcToyeCkq3mIlcC1cVSItR/HJHoj/fPf/7TzTEeDMbS4Kq4vh/XjBOaelCzTmjOduBiVKLPR+azTZnvNl9VLlnMfDZt2Anz1byG/+1b99GRPSPMZ61L9k7V/Z/IEWX7G2Wz4oWKWeYM0cm67ZQMOgoUmupZgSM5vfrqq+75O5ryONJyalN4uvrqq72cYvlCdeTIEVfRKlmy5BnjphKaDRLduqbyodLnwIED3YAxBR3NBBE+k0M4pWSV8zQjhcpumm5PiVclQwAAAPht69atbqrjK664IuJyWqCQo/tV+CXRQUelLs2jrt5DTV2nnj6V1TR/dyTqtdTTVvWvAxpYpgdOaQCUehLjorKfklr4CwAAABceDdjX+BK1SkVaTgtUydE/xuMiHqOjwVuaqUH9fAH1HzZu3NhVaxJC83NrAJQGmMVFfYKDBw9OzKEBAADgHKkTJ7nt2rUr3mUgTVR0du/e7WaQ0EOHwmlZ43USQoO1NHBL4SguClLquQteKnECAAAAQJqcdU0zVmgqQP1rQXwPY8qUKZN7AQAAAECKBx1NpadpBnfs2BHjfS0XKFAg3nU1hZ+CzmeffeYeCAUAAAAAaaJ1LTo62k0PHT6RQDCxgJ7SGpfnnnvOTSE4Z84cq1mz5rkdMQAAAAAkd+uappbu0KGDCyx6Fo7mWT948KCbhU30sCE99EgTCohm1BgwYIC988477gmwwVie7NmzuxcAAAAApHrQad26tZstQ+FFoUXTRqtSE0xQsGXLlhhPgn3llVfcbG133HFHjO3oOTyJfWIuAAAAAKTYZARdunRxr4RMS7hp06ak7AIAAMA7GxrXPq/7q7hmtaUFkyZNsm7dutlff/0VcTm56R/TZ8yYYStWrEjyNnQPW7JkSfvuu+/cP+xfDDp27Oj+Jrp20rBhQ3fu6uC6KB4YCgAAACS2I2jdunVxLqe0H3/80W6//XY3jCIqKireG3cNx+jfv3+KH5OOIwgUadX777/vxtlfqAg6AAAASBINT0iILFmyWL58+eJcTml6YH2pUqXcDMDxzRSs50V++OGH1rJly1S/ZmlBnjx5LEeOHHahIugAAAAg1KoUDFHIlSuXe7TIE088YadPn3afqyKif+HX5FM5c+a0zp07hx4IX65cOcuaNasLFFrn+PHjoe2qVS137twRl/VweD2+ZOnSpaEZfXWDXbdu3dD333rrLStatGho+Wz7i61WrVr2/PPP21133RXvsxoXLVpkGTNmdN8PrFmzxurXr++eAVmpUiVbsGBBjHVWrVplN9xwg5tkS2PW27VrZ7t37z7jmqpVT9ezWbNm7jrKrbfe6io7wXJc7WStWrWyoUOHuu3ruj355JN24sQJ69Wrl7tWRYoUsYkTJ8ZYb+vWrXbnnXe67+s7t9xyS4whJQp1mmRMn1966aX22GOPhf7O4ceu446vCqX19fcUbV/fmTZtml1zzTUu0Opaqnr37bffusnMdJ10vTTmP6URdAAAABAyefJky5Ahgy1ZssRefPFFGzFihL3++usxno1YtWpVN3ZFAUP0r/662f3pp5/cOuPHj7eRI0cmaH8KVBoHEozzXrlypbtZ1vYPHDjg3lO4aNCgQWidc9lffGbOnGktWrRw+w8oTPTo0cMdjx6nos//+OMP95nGs1x33XVWvXp1F9Q0QZeeL6mAEfua6jEtX331lY0bN87d9IvCye+//x5ajsu8efPst99+sy+++ML9PTSp180332yXXHKJffPNN/bggw/aAw88YL/++qv7vkJfs2bN3HX68ssv3X4VMJo3bx6qKA0fPtxdwwkTJtjChQvtzz//tOnTp1ty0PGp/W/58uXuv6W7777bBSn9rXQ869evdxObpTSCDgAAAEJUOVFoKF++vLVt29YeeeSRGCFCN/a68S9durR7iW5qVfVQZUJBoGfPnu5f9RNKlYMg6OhnkyZNrGLFiu4GPHgvPOic6/7i8sEHH5zRtqZqjMb36Hg0m7CC2RtvvOE+GzNmjAs5qrZUqFDB/a7g8Pnnn8cYg1S2bFn3XEldU70uu+yyUDVErXTBclxUkXnppZfcuvfee6/7qXa8xx9/3G27b9++LkgF12vq1KmuMqaAWrlyZXfsClWaHTm4zhqnpPVuu+0297kCmM4tOejvoaCl7Xbt2tWWLVvmQvFVV13lrtF9993nrlFKI+gAAAAgRC1j4RUNVTF+/vln1+okkR7+rhtr3cTqpl2VAwUR3VQnlEKMbtK1D1VvFHyC8KNKhioAWk6u/UWyevVqt6/rr78+xvs6/4CqEzp/fVe+//57d8MePB9SLwUe2bBhQ2i9GjVqnHX/Ov7w7Sg8Ba644ooYj29RC5sCTECtf2o/27lzZ+i41q9f7yo6wfYUlo4cOeKOS+2CqiTVqVPnjHNLDlWqVIlxrBJ+vHovONY0N700AAAALk7ZsmWLsbx48WJX+Rk8eLD7V3xVBaZMmeJaoxLq2muvtf3797tWJ7Vn6SZfIUaTB6hNrlChQq5ykVz7i6ttTZUkjcVJKLXWqaL07LPPnvFZwYIF47xmkegcw6fDVjAJaNxQOAXRSO+pihMcV40aNeztt98+Yz9nqx7FR/uIPY4n0tio8GMLQnPs94JjTUkEHQAAAIRozEe4r7/+2oUMVQ3iGsBfvHhx69evX+i9zZs3J2qfauFSFUCtYLohVlVEs7JpGmrNghbetpYc+4urbS2YXCH2+SuIiSYAUBtW8DzJK6+80t577z3XQqeKSGLoPIMqmWj9MmXKnPN5BMc1depUdw01aUQkCmL6W8c+N60bF4UkVYICqvSphS6tonUNAAAAMVqoNBvX2rVr7d1337XRo0e7cRZxUQjSOqqqqC1KY0mSMqhdrWmqQAShRhUNjfHQDXt40EnK/jQAX9USvfT7tm3b3O9q7xK1UWkyAQ3wj23s2LFu+5p97eGHH7Y9e/a4cTKiZQ3ib9OmjZtQQMfzySefuGfxhIeYSBSO5s6da9u3b3fbTE6qeOXNm9fNtKbB/xs3bnRtgP/4xz9CExbob6qKmWZR07k99NBDZ32Aq8ZnKYxqYgZdL02CELuylJZQ0QEAADhPSn+25Jy3EV0kZZ9roqmjDx8+bLVr13ZVHN0QR6p0BDR4/9FHH3VVjqNHj9pNN93kBp4PGjQoUftVmNEA+fCxOPpd403C30vK/jT2RoPgw2eO00v7VACYNWuWO1+Fg9gUBvRSMFLFRS1uwffUbqYZzTTdddOmTd3xqNqk2c3Cx9REolY7BUrNGFe4cOEYUz+fK027/cUXX7jj0mQDagvUPjT+KKjwaEIJVWc6dOjgjlXhTdNda/xOfMesEKepo3XumkVNVaC0Kup07Ea7NGjfvn2u/1IXPq7yG+C7En0+Mp9tyny3+apyyWLms2nDTpiv5jUca746smeE+ax1yd6puv8TOaJsf6NsVrxQMcucITpZt52SQUeBQlM9K3Akp1dffdU9fyeoJsReTm0KT1dffbWbAhlpgyZOUCWqZMmSZ4ybSmg2oHUNAAAAKUYPrpw9e7abOSzSclqgkKP2M/iF1jUAAACkGA1uV9uUHk4ZaTktoJLjJ4IOAAAAnOBhkslp165d8S4DKYXWNQAAAADeIegAAAAA8A5BBwAAAIB3CDoAAAAAvEPQAQAAAOAdgg4AAAAA7zC9NAAAwHky/qlvz+v+Hh53XbJub9OmTe5J9d99951Vq1bNTUfdqFEj27Nnj+XOndtSS8OGDd3xjBo1KtWOAWkPFR0AAACkOQpVUVFRtmLFCvPR8ePHrXfv3la5cmXLli2bFSpUyNq3b2+//fZbah+aNwg6AAAASFOOHTtmvjt06JAtX77cnnjiCffz/ffft7Vr11rLli1T+9C8QdABAABAyJw5c+zqq692rWiXXnqp3XzzzbZhw4Z41/nqq6+sSpUqljlzZqtbt66tWrUqxucLFy60a665xrJkyWJFixa1f/zjH3bw4MHQ5yVKlLAhQ4a4ikbOnDmtc+fOrkVOqlev7io7ak+Lz4kTJ6xLly6WK1cuy5s3rwsQp0+fDn3+5ptvWs2aNS1HjhxWoEABu/vuu23nzp2hz9V+17ZtW7vsssvccZYtW9YmTpwY+nzr1q125513uuuSJ08eu+WWW1zVKS7169d3FZtwu3btsowZM9oXX3zhjvPTTz912yxfvry7bmPGjLFly5bZli1b4j1XJAxBBwAAACEKIN27d7elS5fa3LlzLV26dHbrrbfaqVOn4lynV69eNnz4cPv2229dUGjRooVrzRKFpObNm9vtt99uP/zwg02dOtUFH4WScC+88IJVrVrVjf9RSFmyZIl7/7PPPrPff//dVTziM3nyZMuQIYNb78UXX7QRI0bY66+/Hvpcx6Mw9f3339uMGTNcSOnYsWPoc+3zp59+so8//thWr15tr7zyigtMwbrNmjVzIenLL790wS579uzuvOKqPik0TZkyJUbY0rmrRU2hL5K9e/e6UJea4518wmQEAAAACFEgCTdhwgQXXhQCdHMfycCBA61JkyahwFGkSBGbPn26q1YMGzbM3fR369bNfa5KyUsvvWQNGjRwYUJVILnuuuusR48eoW2mT5/e/VRVSRWYs1GlaOTIkS4oqEKycuVKt9ypUyf3+b333hv6bqlSpdwx1KpVyw4cOODOS1UUVY9U9QmqTOEBRUFPwUnbF1V7FEg0IUPTpk3POB6du845qGbJO++8Y23atAltI9yRI0dcBUifq6qFc0dFBwAAACE///yzu9lWGNANd3DDH187Vb169UK/q61LQUNVEVEFZdKkSS5MBC9VRxQcNm7cGFovCBjxUTUlfDtvv/126DO1foUHCB2TzuXkyZNuWS1hqjQVK1bMVWYUtMLP6+9//7urwGj2tscee8wWLVoU2pbOYf369W69YN86T4UTVawiHZfCoQJQcIw618WLF7vQF5sqRgpGqv4o/CF5UNEBAABAiMJA8eLFbfz48a7NSoGkUqVKSZ4gQBWTBx54wI3LiU2hI6CZx85GYSh8Frb8+fMnuB1P4UqvIIQo4Gg5OK8bbrjBNm/ebLNnz3ZjZ66//np7+OGHXUudzqFGjRoxglVA24qOjo54XAo1Ou/Ro0e7ao5mWNMrUsjRvufNm0c1JxkRdAAAAOD88ccfbuYvhZyg3UqtV2fz9ddfh0KLBvWvW7fOKlas6JavvPJK1/ZWpkyZRB2LwoMEFRnRJAFxbeebb74545jUJqcWuDVr1rhze+aZZ1yLm2gMUqTQ0qFDB/fS+WvskYKOzkHta/ny5YsziEQ6Lk1YoIkVNMGDgo4mW4gUclR5+vzzz12bHpIPrWsAAABwLrnkEnez/dprr7lWLVUYNDHB2Tz55JNu4gLNtqYB/hrE36pVK/eZxp2oDUyTD6jqoZv6Dz744IzJCGJTqFCwUUjYsWOHG6gfH1VodKwKau+++66ronTt2tV9phCm4KT3fvnlF5s5c6abmCDcgAED3HHpvH/88Uf78MMPQ2FNlRmdk4KL2tTUhqaxOarW/Prrr3Eek6pUug6a6ECtfGoJDA85d9xxhwtcqhQp0G3fvt29Lobptc8HKjoAAADnSaf+tc55G9FFclhK0QxrGqeiG3i1q2msjQbtn21qZ1VKFCoUYjTGZdasWaGKjKadXrBggfXr189VSTQOpXTp0ta6det4t6kZ1LRvhSiFEK2rcBEXVUsOHz5stWvXdlUcHY+qKUGlRuOEHn/8cbdNVWhUqQl/Zo2Ot2/fvm42NgUs7U/XQrJmzeqmhFZou+2222z//v1WuHBh1952tlYzhaQbb7zRrr322hitetu2bXOBS3TNwqm6c7ZrjrOLOh0+510atW/fPjfXuJI8fYu4WJXo85H5bFPmu81XlUv+7//YfDRt2Anz1byGY81XR/aMMJ+1Lhnz+SXn24kcUba/UTYrXqiYZc7w3xv+5JKSQQdIKzTRgypnep5SMDNfYrMBrWsAAAAAvEPQAQAAAOAdgg4AAAAA7xB0AAAAAHiHoAMAAADAOwQdAAAAAN4h6AAAAADwDkEHAAAAgHcIOgAAAAC8kyG1DwAAAOBiMbpHm/O6vx5TP0zW7W3atMk9qf67776zatWq2fz5861Ro0a2Z88ey507t6WWhg0buuMZNWpUqh0D0h4qOgAAAEhzFKqioqJsxYoV5rvx48fbNddcY5dccol7NW7c2JYsWZLah3XBI+gAAAAgTTl27Nh53d/p06ftxIkTllpUGWvTpo19/vnntnjxYitatKg1bdrUtm3blmrH5AOCDgAAAELmzJljV199tWtFu/TSS+3mm2+2DRs2xLvOV199ZVWqVLHMmTNb3bp1bdWqVTE+X7hwoatYZMmSxd3E/+Mf/7CDBw+GPi9RooQNGTLE2rdvbzlz5rTOnTu7FjmpXr26q+yoPS2h3nzzTatZs6blyJHDChQoYHfffbft3LkzRrDQNj/++GOrUaOGZcqUyR3j/v37rW3btpYtWzYrWLCgjRw50u23W7duoXWPHj1qPXv2tMKFC7vv1alTx20vLo8//rj7TmxVq1a1J5980v3+9ttv20MPPeTa7ypUqGCvv/66nTp1yubOnZvgc8aZCDoAAAAIUQDp3r27LV261N1op0uXzm699VZ34x2XXr162fDhw+3bb7+1yy67zFq0aGHHjx93nykkNW/e3G6//Xb74YcfbOrUqS5UdOnSJcY2XnjhBXfzr/E/TzzxRKh167PPPrPff//d3n///QSfg/at4PT999/bjBkzXBtcx44dz/henz597JlnnrHVq1e7oKbzVmibOXOmffrpp/bll1/a8uXLY6yj41bVZcqUKe58/va3v7nz+/nnnyMei4KTziU8LP74449uXQWwSA4dOuTOIU+ePAk+Z5yJyQgAAAAQokASbsKECS68/PTTT5Y9e/aI6wwcONCaNGnifp88ebIVKVLEpk+fbnfeeacNGzbM3ewHVZGyZcvaSy+9ZA0aNLBXXnnFVYHkuuuusx49eoS2mT59evdTVSVVZRLj3nvvDf1eqlQpt79atWrZgQMHYpyDKirBcauao2N/55137Prrr3fvTZw40QoVKhT6/pYtW9x7+hm8r+qOqmB6f+jQoWccyxVXXOECnLarABdUcFTlKVOmTMTj7927t9u+xuog6ajoAAAAIESVCY0XUUBQG5naykQ393GpV69e6HdVIcqXL++qJKKqyqRJk1zACF7NmjVzFaKNGzeG1lOr2dmowhK+HQWGSJYtW+aqSsWKFXPtawpVkc4hfJ+//PKLq6LUrl079F6uXLncuQRWrlxpJ0+etHLlysU4jgULFoQqNuHvP/jgg+49BT0FnWA80Lvvvuvei0QVJlWLFBSDEIikoaIDAACAEAWE4sWLu5nAVFVQIKlUqVKSJwhQFeWBBx5w43JiUxAJaLzL2SiYhM/Clj9//oitdwpSeikIqRqlgKPl2OeQkH3GPhdVmhSkgopTIKgUhR+fgqIoOKpKoza4w4cP29atW61169ZnbF/tewo6atdTKx3ODUEHAAAAzh9//GFr164NTXcsGk9zNl9//XUotOiZOuvWrbOKFSu65SuvvNK1vcXVphWX6Oho91MVlIAmMzjbdtasWePOQ4FBEx+IxhudjSpYGTNmdOOMgnPZu3evO5drr702NDGCjkcTGwTXJ7ZIx6dWPlWVFLwUdNQuly9fvhjfee655+zpp5+2Tz75JEHVLZwdQQcAAACOnuGiMTGvvfaam3VMlRAN2D8bjXXReqqw9OvXz/LmzWutWrVyn6mSoZnYNIj//vvvd1UUBR8N9h8zZkyc21QQULDR+BcFBbVxqZXsbBRSFJJGjx7tWsc0A5wmJjgbtbh16NDBTayg9jvtX2OPNBmDZmgTtayp5Uyzw2nyBQWfXbt2uUkbVIG56aab4ty+1tP2VFXSbG7hnn32WRswYIBrb1Or4Pbt2937QQsckoagAwAAcJ48Mvzdc95GdJEcllJ0U6/xIWozU7uaxqdoIP/ZpnZW9aRr165ufI+mSJ41a1aoIqMAoDEsCkCqgmiMSunSpSO2boXLkCGD27dClEKA1o1vGueAWtU0JkjTOmt9VZTUEtayZcuzrjtixAgXjjSlttrOHnvsMddmFj5WRpMOPPXUU27iBD3nRqFOQU7rxOeOO+5wYU8tb0EIDGhSBgUgfSecgtGgQYPOetyILOq0/mtL4/bt2+cSvMqHQa8jcLEp0ecj89mmzJGn2PRB5ZL/60H30bRhqfeQvZQ2r+FY89WRPSPMZ61L9k7V/Z/IEWX7G2Wz4oWKWeYM/73hTy4pGXRw5ngfPS9H1Zv77rsvtQ/nonLkyBE3WYWepxR7UoaEZgMqOgAAAICZe4aPxvho5jXdRAcP9LzllltS+9CQBAQdAAAA4P+ozU0TMqj1rkaNGm5Ka7Wn4cJD0AEAAAD+b1Y1TR0NP/DAUAAAAADeIegAAAAA8A5BBwAAAIB3CDoAAAAAvEPQAQAAAOAdgg4AAACchg0bWrdu3VJt//Pnz7eoqCj766+/Uu0YLjYlSpSwUaNGmY+YXhoAAOA82TlmxXndX5FnrrG0oGPHji68zJgxI7UPBRcRgg4AAABSxMmTJ12FBkgNtK4BAAAg5MSJE9alSxfLlSuX5c2b15544gk7ffq0++zo0aPWs2dPK1y4sGXLls3q1Knj2s0CkyZNsty5c9vMmTPt8ssvt0yZMtm9995rkydPtg8++MCFHr3C14nkq6++sipVqljmzJmtbt26tmrVqtBnf/zxh7Vp08YdQ9asWa1y5cr27rvvxlj/3//+t3s/S5Ysdumll1rjxo3t4MGDoc9ff/11q1ixott+hQoV7OWXX473eILzCqfqVHiIGzRokFWrVs3efPNN1w6m63fXXXfZ/v37Q985deqUPffcc1amTBl3bYoVK2ZPP/106PPevXtbuXLl3HmVKlXKXfvjx4+HPv/++++tUaNGliNHDsuZM6fVqFHDli5dGvp84cKFds0117jzLlq0qP3jH/+Icd47d+60Fi1auM9Llixpb7/9tvmMig4AAABCFEruu+8+W7JkibuJ7ty5s7sh79SpkwtAP/30k02ZMsUKFSpk06dPt+bNm9vKlSutbNmybv1Dhw7Zs88+68KEQkbBggXt8OHDtm/fPps4caL7Tp48eeI9hl69etmLL75oBQoUsMcff9zdnK9bt84yZsxoR44ccTf4CgW62f/oo4+sXbt2Vrp0aatdu7b9/vvvLggpUNx6660uaHz55ZehsKab+wEDBtiYMWOsevXq9t1337lzU3Dr0KHDOV27DRs2uAD04Ycf2p49e+zOO++0Z555JhRm+vbta+PHj7eRI0fa1Vdf7Y51zZo1ofUVYBSqdG11TXVceu+xxx5zn7dt29Yd8yuvvGLp06e3FStWuGsS7Ft/i6eeesomTJhgu3btcn8vvYLrrhbC3377zT7//HO3noKQwo+vCDoAAAAIUSVAN+KqVpQvX97dcGu5WbNm7oZ5y5Yt7kZcVN2ZM2eOe3/o0KHuPVUgVCGpWrVqaJuqIKgapOCSEAMHDrQmTZqEgleRIkVcqFJwUCVH+w088sgj9sknn9i0adNCQUdVqdtuu82KFy/uvqPqTvi2hw8f7j4XVTYU3l599dVzDjqq2CioKJyIAtjcuXNd0FHgUnhTwAr2o3CmwBPo379/6HdVhXSeCpVB0NG1VwhUFUqCcCnDhg1zQSiYTKJs2bL20ksvWYMGDVww0roff/yxC7C1atVy33njjTdcZctXBB0AAACEqFUsvCWrXr16Lhgo8GjMjVqrwinAqHITiI6Odm1nZ3PDDTe4SosokPz4448x9hlQ9UeBa/Xq1W5Zx6BQpWCzbds2O3bsmDsGtXuJAtb111/vwo3CWdOmTe2OO+6wSy65xLVxqfKhipWqJQEFI7Wane24zkbhJAg5ompWUDHR8es4dWxxmTp1qgsnOsYDBw6441LVKtC9e3e7//77XXuc2vH+9re/ubAUtLX98MMPMdrRTp8+7cLXxo0bXUUsQ4YMrhoWUGCK3ZLnE4IOAAAAzko33mqXWrZsmfsZLnv27DGqNwmZgECtbWppk6D9KiGef/55VxnRlMgKM2o5UxVDgUd0bJ9++qktWrTI/vOf/9jo0aOtX79+9s0334TCkNrHNL4oXHBOkY4rXbp0oda3QPjYmUDs89B1UNAIrkt8Fi9e7CoygwcPdgFNwUvVHIXM8HFAd999t2vXU3VG1Sl9Ry16+vs88MADrh0ttmLFirmgc7Eh6AAAACBEgSDc119/7dqgNDZE1RRVKDTgPTFU5dG64dSCFhftUzfnorEuukkPWqw0UcEtt9xi/+///T+3rCChzzX5QXjAuOqqq9xL43FUmVHrmyoiarv75ZdfXKiIJNJxXXbZZa71TBUhBSvR+JjE0DVU2FErm6oysSmY6TgVygKbN28+43uqqOn16KOPurFIahtU0LnyyitdC54mOoikQoUKrkKkoBq0rq1du9brZxYRdAAAABCisRwKBKoOLF++3FVEVFXQzbXCQfv27d2ygo8GvOvGXa1qN910U7wtXRpHoxtrtbmpWhFfFefJJ59038ufP7+78dfsb61atQoFBs2qpmCgdrQRI0bYjh07QkFHQU3HpJa1fPnyuWUdZxCUVDFR1UPHoMH7aifTpAsKVDrvSFT9UTVIEyNoXW1TY3ESQzO8aQIFjbdR8FMI03GpNU6tdDovXXtVaBREVLVROAuoyqTxOWrD07iiX3/91b799lu7/fbb3efattoONfmAglS2bNlc8FF1S+OC1P6n89XfVWN21MamStjZKk0XMqaXBgAAQIiCjG6qNbD/4Ycftq5du7qZ10TVA33eo0cPd+Os8KGb7aD6EheNh9H3a9as6aojqsrERzOVab8aT7J9+3abNWuWCwfBgH1VL9Te1bBhQzfBQRCCRGNavvjiC7vxxhtdONP3Fcw09kYUAtSepnNR65sG6yu0KDzEReOE3nrrLZs9e3ZoOmu1kSWWpovWtVOVScGrdevWoTE8LVu2dFUaBRVNU60gp++Ht9Zpam1df52XJmbQOSm4icLmggULXHVLFbfq1au7/QQTR4jOWcs6Z03GoL+rwqCvok7HbjhMgzQdoVL33r17YwzIAi4mJfp8ZD7blPlu81XlkvHfAFzopg07Yb6a13Cs+erInhHms9Yle6fq/k/kiLL9jbJZ8ULFLHOG/96gJ5foIv8b7A746siRI24SBQVQVcOSkg2o6AAAAADwDkEHAAAAgHcIOgAAAAC8Q9ABAAAA4B2CDgAAAADvEHQAAACSWzCnbdqf3BZIk/Qg2HPFA0MBAACSWbrDp82OnLI/Dv1ll2bNbRYVlWzbPnXkSLJtC0hr9OSbY8eOuYeppkuXLvT8pKQg6AAAACSzdCfNsi09Ygdqmh3IvD9Zt53hcMxnigA+ypo1q3sQrcJOUhF0AAAAUkD0n6csw9xDdipLlFnyFXSsQI+KybcxIA1Knz69ZciQwaLOsRJK0AEAAEjByk66A8k7Tif2U+IBRMZkBAAAAAC8k6SgM3bsWCtRooT7F4U6derYkiVL4v3+v/71L6tQoYL7fuXKlW327NlJPV4AAAAASP6gM3XqVOvevbsNHDjQli9fblWrVrVmzZrZzp07I35/0aJF1qZNG7vvvvvsu+++s1atWrnXqlWrErtrAAAAAEiZMTojRoywTp062T333OOWx40bZx999JFNmDDB+vTpc8b3X3zxRWvevLn16tXLLQ8ZMsQ+/fRTGzNmjFs3kqNHj7pXYO/eve7nvn37Enu4gDdOHT1kPtsX5e+zJk4ePmk+O3DS3/M7fOyg+ero8ePms/1H/f3bcT+Ei92+//vfgKaijtfpRDh69Ojp9OnTn54+fXqM99u3b3+6ZcuWEdcpWrTo6ZEjR8Z4b8CAAaerVKkS534GDhyoo+bFixcvXrx48eLFixev05FeW7dujTe7JKqis3v3bjt58qTlz58/xvtaXrNmTcR1tm/fHvH7ej8uffv2de1x4U9G/fPPP+3SSy8952nmAAAX7r/gFS1a1LZu3Wo5c+ZM7cMBAKQSVXL2799vhQoVuvCml86UKZN7hcudO3eqHQ8AIO1QyCHoAMDFLVeuXMk7GUHevHndA3x27NgR430tFyhQIOI6ej8x3wcAAACAc5WooBMdHW01atSwuXPnxmgr03K9evUirqP3w78vmowgru8DAAAAwLlKdOuaxs506NDBatasabVr17ZRo0bZwYMHQ7OwtW/f3goXLmzDhg1zy127drUGDRrY8OHD7aabbrIpU6bY0qVL7bXXXjvngwcAXDzU0qxHG8RubQYAIFmCTuvWrW3Xrl02YMAAN6FAtWrVbM6cOaEJB7Zs2WLp0v2vUFS/fn175513rH///vb4449b2bJlbcaMGVapUqXE7hoAcBFTwBk0aFBqHwYA4AIRpanXUvsgAAAAACDVxugAAAAAwIWAoAMAAADAOwQdAAAAAN4h6AAALngNGza0bt26Jfj7kyZN4kHUAOA5gg4AAAAA7xB0AAAAAHiHoAMASNGWskceecS1lV1yySXumWvjx48PPWg6R44cVqZMGfv4449D6yxYsMA9kFrPzSlYsKD16dPHTpw4Efpc6+rh1NmzZ3ef64HUsR09etR69uzpHmCdLVs2q1Onjs2fP/+8nTcAIPURdAAAKWry5MmWN29eW7JkiQs9f//73+1vf/ube6D08uXLrWnTptauXTs7dOiQbdu2zW688UarVauWff/99/bKK6/YG2+8YU899VRoe7169XJh6IMPPrD//Oc/LsBoO+G6dOliixcvtilTptgPP/zg9te8eXP7+eefU+EKAABSAw8MBQCkaEXn5MmT9uWXX7pl/Z4rVy677bbb7J///Kd7b/v27a4yo2Aya9Yse++992z16tUWFRXlPn/55Zetd+/etnfvXheGLr30UnvrrbdceJE///zTihQpYp07d7ZRo0bZli1brFSpUu5noUKFQsfSuHFjVykaOnSom4xAVaa//vorVa4LACDlZTgP+wAAXMSqVKkS+j19+vQuqFSuXDn0ntrZZOfOnS7g1KtXLxRy5KqrrrIDBw7Yr7/+anv27LFjx465VrRAnjx5rHz58qHllStXukBVrly5M9rZtG8AwMWBoAMASFEZM2aMsawQE/5eEGpOnTqVLPtTKFKgWrZsmfsZTuN6AAAXB4IOACDNqFixomtdU1d1EIC++uorN2mB2tNUvVFI+uabb6xYsWLuc1V51q1bZw0aNHDL1atXdxUdVYiuueaaVD0fAEDqYTICAECa8dBDD9nWrVvdpAVr1qxxEw4MHDjQunfvbunSpXMVmfvuu89NSDBv3jxbtWqVdezY0X0WUMta27Zt3cxs77//vm3cuNFNhDBs2DD76KOPUvX8AADnDxUdAECaoemgZ8+e7YJM1apVXQVHwaZ///6h7zz//POuPa1Fixau0tOjRw83UUG4iRMnupna9JlmctOsb3Xr1rWbb745Fc4KAJAamHUNAAAAgHdoXQMAAADgHYIOAAAAAO8QdAAAAAB4h6ADAAAAwDsEHQAAAADeIegAAAAA8A5BBwAAAIB3CDoAAAAAvEPQAQAAAOAdgg4AAAAA7xB0AAAAAJhv/j/FBmsdm/UJ/wAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.DataFrame({\"model\": accuracies}, index=model_ids)\n",
    "\n",
    "print(df)\n",
    "plt.rcParams['figure.figsize'] = (10,6)\n",
    "ax = df.transpose().plot(kind=\"bar\", rot=0)\n",
    "ax.legend(loc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The huge transformer models that have become so popular in NLP, are often hard to deploy. Sometimes they're too slow to process enormous amounts of data cost-effectively, sometimes they need more memory than is available. In this notebook, we've explored some more efficient transformer models. An evaluation on intent classification shows that they can often hold their own against their larger competitors. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
